[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Unashamed Curiosity",
    "section": "",
    "text": "Dachs2Num\n\n\n\n\n\n\n\nML\n\n\nAI\n\n\nNLP\n\n\nWord2Vec\n\n\n\n\nDiscover how to make computers understand the awesomeness of dachshunds. An explainer of Word2Vec.\n\n\n\n\n\n\nApr 29, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTo be or not to be\n\n\n\n\n\n\n\nGoals\n\n\nIdentity\n\n\n\n\nWhat I learned from trying too hard: How to start and stick to a new endeavor by shifting your perspective from goal-based to identity-based efforts.\n\n\n\n\n\n\nApr 15, 2020\n\n\nKay Kozaronek\n\n\n\n\n\n\n  \n\n\n\n\nCOVID-19, boredom and artificial intelligence\n\n\n\n\n\n\n\nCovid\n\n\nAI\n\n\nPhilosophy\n\n\n\n\nDiscovering the hidden blessing of the pandemic, finding time for what matters and embarking on a journey to becoming a data scientist.\n\n\n\n\n\n\nMar 25, 2020\n\n\nKay Kozaronek\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/test-post/index.html",
    "href": "posts/test-post/index.html",
    "title": "My Post",
    "section": "",
    "text": "This is an exciting first header\n\nLet’s write some stuff"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/covid19-boredom-and-artificial-intelligence/index.html",
    "href": "posts/covid19-boredom-and-artificial-intelligence/index.html",
    "title": "COVID-19, boredom and artificial intelligence",
    "section": "",
    "text": "Smelly Vagabounds(,) and Thinkers\n\nI highly recommended listening Epic Mountain while reading this piece.\n\nIt all started last year when I began reading “The master algorithm” by Pedro Domingos. The smell of sweaty hungover travelers was thickening the air in the Columbian 10-bed dorm I found shelter in. Although I would later find out how little I actually understood, a profound feeling of understanding the complex world we have to navigate, creeped up my sleeve as I kept on reading about artificial intelligence, algorithms, and philosophy. Wait a moment - philosophy? Yeah, philosophy. Many brilliant thoughts that Domingos writes about started out their journey in the head of a thinker’s curiosity-driven pursuit of knowledge - ergo philosophy. As you might guess the epiphanic feeling of understanding “what the heck was going on in the world of tech” didn’t last long. Soon enough I was not only surrounded by smelly vagabonds but also by heavily complex thoughts ranging from philosophy to computer science and back to the world of mathematics. A little lost and confused I decided to put down the book and get to grips with the basics first. That’s when I decided to devote the majority of my time reading and studying philosophy. The next image is a depiction of me. Literally, go ask my friends!\n\n\n\n\n\nFast-forward, December 2019. Over the last couple of months, I devoted a good chunk of my time pondering philosophical questions, when I got curious again. Now that I had at least a basic understanding of philosophy, I remembered that there was something else that bugged my mind. Algorithms, Programming and Mathematics. I should start getting technical right? - Yep. And so I did. Well, at least I tried. Getting into programming and refreshing some of my mathematical knowledge wasn’t as easy as previously thought.\nWeeks later, near the end of February 2020, I finally managed to pull through the strenuous task of finishing my first online Machine Learning Course. Did I enjoy it? Well… It was tough, and I was nowhere near to programming my first line in Python, which by now I knew was the go-to programming language for Machine Learning. But then again, did I even try learning to write any Python code? Mhm, let’s skip that part. Being the problem solver that I am I tried to find a better, more satisfying way to get into AI. I was determined to learn. Hell, so determined that I walked around my new hometown Madrid for 3 weeks on end to talk to people who’d already been navigating the complex world of machine learning, data science and artificial intelligence. After several failed attempts to find a suitable environment to learn, I got lucky. The Immune Coding Institute offered a Data Science Bootcamp, and I was accepted to partake! I jumped on that chance and grabbed it by the balls.\n“Finally, every day I’ll be surrounded by like-minded people with whom I’ll share a common goal: Becoming a Data Scientist.” - At least that’s what I thought. Invigorated, enthusiastic and full of joy I completed the first two sessions of the Bootcamp only to wake up the next day to realize that the Corona Crisis had reached Madrid. Another setback to my efforts of becoming a Data Scientist? No, not this time!\n\n\nRegnosis: The Corona Crisis, a blessing in disguise?\nLike many of you out there, I am trapped at home, prohibited from going out. It’s shit, right? Well, actually, no! Why’s that, you might ask. Here’s the thing, let’s make a thought experiment, good ol’ philosopher-style. In hindsight, what will you think of today’s situation 6 months from now? Go ahead, take a couple of minutes, maybe write down your thoughts, it’s a great way to stay sane!\n\n\n\n\n\nHere’s what I think. 6 months from now many of us will look back at today and revisit the boredom that they experienced and how they went about changing their relationship to it:\n“Wow, I never thought that the Crisis was such a blessing. All the time it freed up for me. I was so stressed every day, running from one obligation to another, depriving myself of spending time with my friends and family. But then, when I was forced to stay home, it kind of clicked. First, I was bored and tended to binge on Netflix, but that got boring quite quickly too. The realization came late, but when I realized that this crisis was a reminder of all that’s important in life, I quickly gained control over the situation. I remember the day that I finally found time to do what I was longing to do for so long. I eventually started”XYZ” (writing/ playing music/ learning a new language) - fill in whatever dream of yours you wanted to pursue but lacked the time to do. Damn, if it wasn’t for the crisis I might still be wondering what it was like to do “XYZ”.\nPeople who realize this fast enough will be able to act upon the newly freed time and will profit tremendously in whatever area they choose to apply it to.\n\n\nCall to arms\nFor those of you who have not already changed the Browser Tab to start another binging marathon on Netflix, firstly, thanks. Secondly, here’s the deal! I want you to take 5 minutes and think of everything that you always wanted to do but never “found time for”. Then pick one thing and start acting on it.\nAs I know that it’s always easier said than done, I encourage you to follow my lead. I’ll have a weekly posting schedule where I’ll blog about my journey of becoming a Data Scientist. I want you to engage with me and comment on what you did in your personal project.\nOk, that’s basically it. If you’re done, feel free to write a comment down below or hit me up on LinkedIn to help each other stay motivated on our journey.\n\n\nGratefulness:\nBefore ending this post I would like to give a big thank you to the following people for inspiring me to take action and embark on a crazy journey to becoming a Data Scientist:\n\nAndrei Neagoie and Daniel Bourke for being kick-ass instructors that know how to ignite their students’ passion for a subject as complex as machine learning. Check out their Course if you’re interested in Data Science\nImmune Coding Institute for believing in me and giving me a chance to study Data Science at their wonderful facilities\nPedro Domingos who wrote the challenging book without whom I wouldn’t have had the curiosity to dive deeper and gather the necessary knowledge to understand his brilliant work\nMy girlfriend for preparing world-class scrambled eggs to free up time for my projects\nMy family for supporting me mentally and financially on my crazy adventures\nEpic Mountain for making inspiring music that transforms your brain into a pondering - machine!"
  },
  {
    "objectID": "posts/covid19-boredom-and-artificial-intelligence/index.html#regnosis-the-corona-crisis-a-blessing-in-disguise",
    "href": "posts/covid19-boredom-and-artificial-intelligence/index.html#regnosis-the-corona-crisis-a-blessing-in-disguise",
    "title": "COVID-19, boredom and artificial intelligence",
    "section": "Regnosis: The Corona Crisis, a blessing in disguise?",
    "text": "Regnosis: The Corona Crisis, a blessing in disguise?\nLike many of you out there, I am trapped at home, prohibited from going out. It’s shit, right? Well, actually, no! Why’s that, you might ask. Here’s the thing, let’s make a thought experiment, good ol’ philosopher-style. In hindsight, what will you think of today’s situation 6 months from now? Go ahead, take a couple of minutes, maybe write down your thoughts, it’s a great way to stay sane!\n\n\n\n\n\nHere’s what I think. 6 months from now many of us will look back at today and revisit the boredom that they experienced and how they went about changing their relationship to it:\n“Wow, I never thought that the Crisis was such a blessing. All the time it freed up for me. I was so stressed every day, running from one obligation to another, depriving myself of spending time with my friends and family. But then, when I was forced to stay home, it kind of clicked. First, I was bored and tended to binge on Netflix, but that got boring quite quickly too. The realization came late, but when I realized that this crisis was a reminder of all that’s important in life, I quickly gained control over the situation. I remember the day that I finally found time to do what I was longing to do for so long. I eventually started”XYZ” (writing/ playing music/ learning a new language) - fill in whatever dream of yours you wanted to pursue but lacked the time to do. Damn, if it wasn’t for the crisis I might still be wondering what it was like to do “XYZ”.\nPeople who realize this fast enough will be able to act upon the newly freed time and will profit tremendously in whatever area they choose to apply it to."
  },
  {
    "objectID": "posts/covid19-boredom-and-artificial-intelligence/index.html#call-to-arms",
    "href": "posts/covid19-boredom-and-artificial-intelligence/index.html#call-to-arms",
    "title": "COVID-19, boredom and artificial intelligence",
    "section": "Call to arms",
    "text": "Call to arms\nFor those of you who have not already changed the Browser Tab to start another binging marathon on Netflix, firstly, thanks. Secondly, here’s the deal! I want you to take 5 minutes and think of everything that you always wanted to do but never “found time for”. Then pick one thing and start acting on it.\nAs I know that it’s always easier said than done, I encourage you to follow my lead. I’ll have a weekly posting schedule where I’ll blog about my journey of becoming a Data Scientist. I want you to engage with me and comment on what you did in your personal project.\nOk, that’s basically it. If you’re done, feel free to write a comment down below or hit me up on LinkedIn to help each other stay motivated on our journey."
  },
  {
    "objectID": "posts/covid19-boredom-and-artificial-intelligence/index.html#gratefulness",
    "href": "posts/covid19-boredom-and-artificial-intelligence/index.html#gratefulness",
    "title": "COVID-19, boredom and artificial intelligence",
    "section": "Gratefulness:",
    "text": "Gratefulness:\nBefore ending this post I would like to give a big thank you to the following people for inspiring me to take action and embark on a crazy journey to becoming a Data Scientist:\n\nAndrei Neagoie and Daniel Bourke for being kick-ass instructors that know how to ignite their students’ passion for a subject as complex as machine learning. Check out their Course if you’re interested in Data Science\nImmune Coding Institute for believing in me and giving me a chance to study Data Science at their wonderful facilities\nPedro Domingos who wrote the challenging book without whom I wouldn’t have had the curiosity to dive deeper and gather the necessary knowledge to understand his brilliant work\nMy girlfriend for preparing world-class scrambled eggs to free up time for my projects\nMy family for supporting me mentally and financially on my crazy adventures\nEpic Mountain for making inspiring music that transforms your brain into a pondering - machine!"
  },
  {
    "objectID": "posts/to-be-or-not-to-be/index.html",
    "href": "posts/to-be-or-not-to-be/index.html",
    "title": "To be or not to be",
    "section": "",
    "text": "To be or not to be\n\nWhat I’ve learned from trying too hard.\n\nIn what follows I’ll explain how to start a new endeavor and stick with it. More concretely the notion of identity-based and goal-based achievement will be discussed in consideration of how to develop the necessary mindset that results in sustainable, repeatable and pleasurable habits which in turn will yield a fruitful ground for achievement.\nA few weeks back an excited youngster sat at his desk trying to make sense of a bad situation and assign it some meaning. He would go on to formulate his ideas in a rush of feel-good endorphins. The pounding of keys would finally end after a short distance writing-sprint. After adding a witty title here and some funny pictures there, his first written piece was ready to be published. That should do for a first blog post, right? Right. Click - post - share - ask for feedback - get lavish praise by friends - wait until next week - hope to be blessed with random sparks of creativity - write again - repeat - become a writer - mission accomplished! As if, haha! Now who am I talking about, you’re asking? - Drumroll - That someone was me. Now some of you might ask what’s your point here? Isn’t that exactly how it works? Well no, here’s why.\n\n\nGoal-Based - Efforts\nThe above-described situation perfectly demonstrates how goal-based efforts work. As I set out on my journey the only thing on my mind was the goal: “Write a blog”. Now, a goal in and of itself is not a bad thing, but neither is it good, a goal is just a goal.\n\n\n\n\n\nGoal-Based and Identity-Based EffortsAs seen in the diagram above, goal-based efforts work from the outside in. The typical goal-setting process starts out by defining what it is that you want to achieve. Let’s say your goal is to lose weight. You begin to produce certain outcomes, like losing a pound here and there due to training and eating right. The actions you take might or might not lead to habits, that eventually become automated as you go to the gym every Monday, Wednesday, and Friday. Finally, if you’re in luck, you’ll finally incorporate being sporty, as part of your life. This process might take weeks, months or even years in some cases, but let’s be honest, we’ve all tried and failed at something like this, which leads makes me think, that there has to be a better way. Back to the story.\nIn my last blog post I proudly claimed, that I would upload 1 post per week in order to capture my journey of becoming a Data Scientist and Writer. Well, did you see me write anything in 3 weeks? Exactly, neither did I. Why? Mostly, because the anticipated exhilarating bursts of creativity didn’t appear according to plan, which quickly let me question my ability to write. This is by no means a valid excuse, which is precisely why I’m all the more thankful for what followed!\nA little beaten down by reality, an old friend of mine, Tom, taught me a valuable lesson. He asked me the following: Are you a writer or do you want to become one? In essence: “to be or not to be?”\n\n\n\n\n\nFor those who want to understand the profound contemplation behind this quote, and not just my misuse of it, check out this fun Youtube Video!\nTom went on to explain the problem with goal-based efforts. Setting goals, followed up by painstakingly following down that road, without ever stopping and asking “Why?”, is the equivalent of going on a hike without any shoes. In short, the journey will be very painful, unenjoyable and over, rather quickly. You’ll give up on your hike before it even began.\n\n\nIdentity-based efforts:\nAnother approach that works much better than goal-based efforts, is to work from the inside out and focus on your identity first, Tom said. A bit confused I asked: “what do you mean by that?” Patiently, he elaborated: Before starting out on a new journey, ask yourself these fundamentals:\n\nWhy do I want to do this? What’s the purpose of all this?\nDo I see myself doing this, 10 years from now?\nIs this something I enjoy or can come to enjoy?\nAm I willing to invest at least a couple of minutes, every day?\n\nIf the answer to most of those is “yes!”, or “I don’t know, but I’m sure as hell willing to find out”, congratulations! You’re now officially a “(fill in whatever it is that you want to be)”. You’re probably still a bloody amateur, but that’s alright.\nI pondered these questions in the back of my mind, as Tom continued teaching. “You have now started to grow. You’ve incorporated a certain topic as an integral part of your life! This is a great leap forward because now, you don’t have the pressure to reach a certain goal anymore. Shifting your perspective away from goals will allow you to view every step onwards as an opportunity to learn. You’ll be able to improve who you already are, rather than trying to become someone. This way you recognize your own imperfections and take the actions needed to even them out. Soon enough, this newfound identity you’re shaping will result in habits that positively reinforce your growth. Your efforts will materialize in outcomes and you’ll lose weight, write something or do whatever it is that you wanted to do in the first place. In summary, you’ll set yourself up for success. You might not achieve one or another thing in the short run, but you’ll never quit on your new endeavor because now it’s part of who you are!” Woah. Let that sink for a moment. It took me 2 weeks to internalize this!\n\n\nConclusion:\nSo, what’s the main takeaway and how does it all tie back to my efforts of writing blogs? What Tom taught me is to “be”, rather than “become”. The hardest part about setting out on a new journey is starting and setting yourself up for success. By accepting that you are already a writer, a runner etc. you’ll eliminate the pressure of becoming that someone. This, in turn, allows you to shift your perspective towards improving yourself. In short, don’t try hard, try smart. Remember, the following:\n\nGoal-based efforts can only get you so far\nIdentity-based efforts are a surefire way to succeed in the long run\nAsk “Why?” before you start\nBuild habits and results will follow\n\n\n\nGratefulness:\n\nThank you, Tom Munk, for opening my eyes to this fundamental truth about any effort whatsoever in life. I’m back on track and I don’t plan on derailing. I owe you a lot and I’m planning on repaying it fully!\nThank you, Andrei Neagoie, for teaching me about the truths about learning, habits and compound learning."
  },
  {
    "objectID": "posts/word2vec/index.html",
    "href": "posts/word2vec/index.html",
    "title": "Dachs2Num",
    "section": "",
    "text": "Word2Vec is one of the most well-known word embedding algorithms. Although it has been succeeded in efficacy by more recent transformer-based algorithms like BERT and GPT, Word2Vec remains a valuable algorithm to understand. It was the first algorithm to produce word embeddings that captured both syntactic and semantic relationships between words, famously demonstrated by the “king - man + woman = queen” analogy. This made a profound contribution to natural language processing and kicked off a swath of impressive developments. In this post, we will take a deep dive into the nuts and bolts of Word2Vec. But before we do that, let’s first review why word embeddings are crucial for NLP tasks."
  },
  {
    "objectID": "posts/word2vec/index.html#intro",
    "href": "posts/word2vec/index.html#intro",
    "title": "Dachshund to Numbers",
    "section": "",
    "text": "Word2Vec is one of the most well-known word embedding algorithms. Although it has been succeeded in efficacy by more recent transformer-based algorithms like BERT and GPT, Word2Vec remains a valuable algorithm to understand. It was the first algorithm to produce word embeddings that captured both syntactic and semantic relationships between words, famously demonstrated by the “king - man + woman = queen” analogy. This made a profound contribution to natural language processing and kicked off a swath of impressive developments. In this post, we will take a deep dive into the nuts and bolts of Word2Vec. But before we do that, let’s first review why word embeddings are crucial for NLP tasks."
  },
  {
    "objectID": "posts/word2vec/index.html#word-embeddings",
    "href": "posts/word2vec/index.html#word-embeddings",
    "title": "Dachshund to Numbers",
    "section": "Word embeddings",
    "text": "Word embeddings\n[NOTE This section could definitely be more elaborated from a theoretical point. Also, better illustration and more examples could help.]\nWord embeddings (also called as vectors, representations) are high-dimensional (usually 50-1000 dimensions) dense vectors over which you can do all kinds of arithmetics and most importantly encode similarity. Arithmetics like the notories example “king–man+woman=queen” or “good-best+bad=worst”. These two examples illustrate how semantic (meaning) and syntactic (grammar) similarities are encoded in the word vectors.\nThe whole idea of word embeddings is based on distributional semantics which is the concept of representing a word based on its usual context. This idea waw popularized by a linguist, J. R. Firth, with his famous quote “You shall know a word by the company it keeps” in the 1950s. “The basic idea of distributional semantics can be summed up in the so-called distributional hypothesis: linguistic items with similar distributions have similar meanings.”\n\n\n\n\n\nWhat is a reasonable alternative to dense word representations? One-hot-encoding (OHE) might first come to your mind which was the way to go in traditional NLP. When using OHE, you basically treat words as discrete, atomic symbols. This way of representing words is usually refered to as sparse, in contrast to dense, distributed representation (like Word2Vec).\nLet’s look at a toy example to illustrate the differences better. Let’s say I want to represent the words: “Chinese crested dog”, “Dachshund”, “Bolognese”, “Labrador” and “Newfoondland”. Using sparse, OHE representation, we would end up with the below word vectors. The individual words have their own dedicated dimension and if you were to calculate similarity between of these words vectors e.g. dot product, you end up with 0.\n\n\n\n\n\nLet’s see how we could represent these words in a dense way. How about we define 2 dimensions that are suitable to capture the meaning of these words to some extent. I think the body size (y-axis) and the fur size(x-axis) are good candidates for our goal. Below you can see my estimation of the word vectors.\n\n\n\n\n\nHere is how the word vectors compare to each other. For the dense representation, distributed, representation only 2 dimensions are used in contrast to the 5 dimensions using sparse, OHE representation. In my opinion, this toy example illustrates how dense vectors capture meaning. Using the very simple distance metric, the taxicab distance (also known as L1 and Manhattan distance), we can clearly see that a dachshund is more similar to bolognese (d=0.88) than to a Newfoondland (d=4.78) because the distance between the two is smaller. Using sparse representation, you would be not able to calculate different degrees of similarity. All sparse vectors would have 0 similarity because the vectors are orthogonal to each other.\n\n\n\n\n\nNOTE: though in this toy example, the dimensions of the dense vectors have a direct interpretation (body and fur size), this is not the case in a real application. There are papers attempting to interpret word vector dimensions in one way or another. For example, in [7] they attempted to extract the ‘gender’ dimension (and debias the word vectors, more about this in section ‘Other Notes’)\nBack to the question of why we care about (good) word embeddings. We could not only decrease the dimensionality of the representation which may have its own regularization effect but we did in a way that we can now compare them and calculate similarities. This way of encoding information seems a lot mor informative than the sparse one! Intuitively, it just makes sense that any downstream application (chatbots, named entity recognition, text summarization) would prefer making use of this information. Isn’t this fascinating? We have assessed the importance of word vectors, now let’s say how to make one!"
  },
  {
    "objectID": "posts/word2vec/index.html#the-word2vec-algorithm",
    "href": "posts/word2vec/index.html#the-word2vec-algorithm",
    "title": "Dachshund to Numbers",
    "section": "The Word2Vec algorithm",
    "text": "The Word2Vec algorithm\nHere, better and more illustrations would be welcome. Some ideas in particular: sliding over the text, show dot products for a given context size with examples for both CBOW and Skip-Gram\nIn summary, Word2Vec beat all previous benchmarks and managed to cut down the on training time vastly which allowed the authors to train on a larger corpus than it was possible before. They used the Google News, a Google proprietary, dataset which contains ca. 6B tokens and has a vocabulary size of 3M. For reference, the Oxford English Dictionary contains over 600,000 words and Phrases [4]. What comes to my surprise is that only a few epochs are needed to train the model. In the paper they used 1-3 epochs. Probably this is enough because the 6B token long dataset containts ca. 6B samples which is quite a number. FUN FACT: Word2Vec was originally implemented on CPUs. It was 2013 and GPUs were already on their way revolutionizing AI but still a year after the seminal paper of Krizhevsky et. al. I guess there were strong reasons why they did all this on CPUs. At this point, I think it is important to note that Word2Vec was first introduced in [1] and then further developed in [2].\nIt is now time to give a one sentence description of Word2Vec:\n\nWord2Vec is a neural network-based pre-training, unsupervised algorithm that produces word embeddings, trained on a large corpus of text data which can be used as input features for various natural language processing tasks.\n\nThis is quite a dense description which is worth elaborating in more detail. It is important to highlight that Word2Vec is an NN-based algorithm because around the time of its publication there were other, non-NN-based algorithms like count-based methods e.g. co-occurence matrix factorization methods and GloVe [9]. It is a pre-training algorithm because its resulting word embeddings is mostly used for downstream NLP tasks where these embeddings are either frozen or jointly optimzied in the downstream task. It is also unsupervised which means that no human supervision like labels are being used during training. The supervision comes from the data itself i.e. how words follow each other.\nThe success of Word2Vec was based on multiple innovations: 1. Two new training algorithms: - Continous Bag-Of-Word (CBOW) - Skip-Gram 1. Two tricks optimizing computational efficiency - Negative Sampling - Hierarchical Softmax\nHere, I am using a somewhat sloppy description because strictly speaking Negative Sampling and Hierarchical Softmax are also part of the training algorithm. I just refer to them as “computational tricks” because, to me, they do not really deal with high-level concepts like how to relate context to a specific (center) word but more about the nuanced implementation details of the loss function. I think later this taxonomy will make sense.\nFirst, let’s review the easier of the two training algorithms, CBOW.\n\nContinous Bag-of-Words (CBOW)\nIn both CBOW and Skip-Gram, we differentiate between center and context words. The context words around the center word are limited by a context window, which was exactly 4 in the paper. This means that there are 4 context words preceeding (history) and 4 (future) following the center word. The choice of this parameter was not explained in the paper and one might actually want to tune it.\nLet’s look at a specific example but instead of using 4, let’s use a context window of 2 words. Here, the center Word “ChatGPT” is surrounded by the context words “GPT-4”, “outperforms”, “in”, “general”.\n\n\n\n\n\nThe difference between CBOW and Skip-Gram is what we actually want to predict. While CBOW uses the context words to predict the center word, Skip-Gram does the opposite and predicts the context words from the center word.\nBy now, we have a good high-level overview of Word2Vec. It is time to move to the tecnhical implementation of CBOW. We will first look at the trainable weight matrices and an illustrative forward and backward pass. Later, we will interpret the more math-heavy part, the loss function.\n\nIllustrated Forward and Backward Pass\nThere are 2 trainable weight matrices: the input and output matrix. N denotes the number of embedding dimensions, V the vocabulary size. N is usually 50-300 and V in the Google News Dataset, as mentioned before, 3M. The input weight matrix is tasked with generating the input embedding for the task, while the output matrix will produce the predictions i.e. probability distribution over the entire vocabulary. This will make more sense in a bit. For now what is crucial to understand, is that each token has an embedding in the input word matrix, this will be its context embedding and an embedding in the output word matrix which is its center embedding in. In other words, each word has 2 embeddings. The tokens in the Vocabulary need to be mapped to indices, so we can look up their embeddings in the input or output matrices using indexing. FUN FACT: Zyzzyva is an actual word and refers to a type of weevils. I looked up on Google what is the last item in English dictionaries.\n\nLet’s put all we know together and a little more by illustrating the forward and backward pass! Below, we use the same example of center and context words. Using the token-index mapping, we look up the indices of the center and context words. The 4 context words are taken from the input matrix and stacked horizontally. To get the average context, we take the row-wise average of the context word embeddings. Using the output (center) matrix, we calculate the dot product between each word in the vocabulary and the averaged context embedding. This is a matrix-vector multiplication. In the resulting score vector each item refers to the similarity (dot product) between the avereage context and the words in the vocabulary. To get a probability distribution over the vocabulary, we apply softmax normalization. The y_true vector will be OHE where the single non-zero element is at the index of the center word. Finally, we apply Cross-Entropy loss and apply backprop to calculate the gradients.\n\n\n\nObjective Function Explained\nNow that we have a better understanding of the forward and backward pass of CBOW, let’s look into the math part. Actually, the math of the “vanilla” Word2Vec is not too convoluted, we just need to dissect the objective function. Quick disclaimer: you won’t find these functions in the original paper. Here, I am making use of [2] and [3] but I make an effort at simplyfying the notation and provide a more detailed explanation.\nOur goal is to maximize the data likelihood i.e. to maximize the probability of a center word \\(w_{t-1}\\) conditioned on the context words in context window of c \\(w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c}\\), a given parameter set of \\(\\theta\\) which is the context and center embedding matrices. T denotes the tokens of the dataset i.e. all the words in the dataset.\n\\[ Likelihood = L(\\theta) =  \\prod_{t=1}^{T} P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c}; \\theta) \\]\nHow do we calculate \\(P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c})\\)? We are yet to inject the word embeddings into these functions because now they refer to words. In section “Illustrated Forward and Backward Pass”, we said that each word has a corresponding context (input) and center (output) word vector. Let’s denote the context word matrix as \\(K \\in \\mathbb{R}^{n \\times |V|}\\) where \\(k_{i}\\) refers to the i-th column of K meaning the context embedding of word at index i. The center word matrix is denoted by $ U ^{|V| n}$ where center word embeddings are stacked row-wise. Similarly, \\(u_{i}\\) refers to the i-th row of U indicating the center embedding of word at index i.\nLet’s now write up how to express \\(P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c})\\) using the contex and center word embeddings and then explain it in more detail. Below \\(u_{t}\\) refers to the context embedding of the context word at position t. \\(\\bar{k_{c}}\\) refers to the average context embedding of the context words. By taking the dot product between these two, we get a similarity metric (an unnormalized one) between the center word and the average context. The higher the dot-product the more similar the vectors are i.e. the vectors point to a similar direction. By calculating the dot product between all center word embedding and the average context embedding in the vocabulary (the denominator) and applying softmax, we get a probability distribution over the entire vocabulary.\n\\[P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c}) = \\frac{exp(u_{t}\\bar{k_{c}})}{\\sum_{w=1}^{V}exp(u_{w}\\bar{k_{c}})}\\]\nIn machine learning, it is easier to work with the dual problem of maximizing likelihood which is minimizing the negative log likelihood. Using this trick makes the optimization easier. If you want to know why, I recommend you reading this thread. Take the log of the product term results in the summation of logs. By convention, we optimize for the average log-likelihood, hence taking \\(\\frac{1}{T}\\).\n\\[ J(\\theta) = -\\frac{1}{T} \\log L(\\theta) = -\\frac{1}{T} \\sum_{t=1}^{T} \\log P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c}; \\theta) \\]\nOkay, so now we got a probability distribution over the entire vocabulary which is the prediction of the model but how do we relate this to the ground truth? The cost function does not (yet) include the y_true term though we use it for the cross-entropy loss calculation in the forward pass chart! I just left out this part from the cost function definition to not to make the formula too difficult to understand. The correct loss function for a given sample (a center word and its context) J is:\n\\[J = -\\sum_{w=1}^{V} y_{w}log(\\hat{y}_{w})\\]\nwhere\n\\[\\hat{y} = \\frac{exp(u_{t}\\bar{k_{c}})}{\\sum_{w=1}^{V}exp(u_{w}\\bar{k_{c}})}\\]\nSince the target, y vector is OHE, J reduces to where the i index refers to the target element of the target vector e.g. the index of “ChatGPT” as in the example:\n\\[J = -y_{i}log(\\hat{y_{i}})\\]\nIf you feel like you can extend the formula to contain all training samples of the dataset but this is pretty much all about the CBOW version of Word2Vec except one last thing. In the next section, we will review the training algorithm again and point out its computation complexity and the methods Negative Sampling and Hierarchical Softmas that serve as remedies ot this issue.\n\n\n\nSkip-Gram\nSkip-Gram flips the role of the context and center words. Here, the center word is used as input to predict the context (output) words. The forward and backward pass are illustrated below in a similar fashion as for CBOW. Observe the differences between the two architectures. Here, the input matrix is the one containing the center word embeddings, the output contains the context ones. We use the center word representation as input to calcute scores for the context outputs. Just like in CBOW, we make a softmax prediction over the entire vocabulary but we contrast these predictions against the multiple target context words which is indicated by the grey box on the right. In this example, we predict the 4 context words from the center.\n\n\n\nSkip-Gram\n\n\nFor ease of notation, only a single sample’s training objective will be detailed. Let’s first start with the likelihood function again. Here, we aim to maximize the probability of context words conditioned on the center word. If we denote context lenght with c, there are 2c context words of which probabilities we aim to maximize (c history and c future context words).\n\\[Likelihood=L(\\theta) =\\prod_{j=1}^{2c}P(w_{j} \\mid w_t; \\theta)\\]\nIt is important to note that the formula implies that we make a strong conditional independence assumption. Given a center word, all output words are completely independent (hence you can simply multiply the probabilities). This is definitely not true. Just think of simple grammar. If you have a noun center word followed by a verb context, then the upcoming context is quite unlikely to be a verb again. To give you an example, if you have the word Chelsea (a famous football club and also a district in London) as the center followed be played context then the next context is less likely to be a verb like jump and more likely to be an adjective like marvelously. Seeing this example you may even give semantical arguments why the conditional independence does not hold like nouns more related to football e.g. game is more likely to come up than academics related nouns e.g. conference. Nonetheless, Skip-Gram manages to work well in practice, thus we may be okay with this. By the way, it is the very same assumption that the Naive Bayes Classifier model makes. This naive assumption lets us to break out probabilities so we don’t have to deal with figuring out the joint distributions.\n[NOTE] I think an elaboration of this point may be useful\nThe softmax is used again to get the probability distribution over the vocabulary. Below \\(k_{t}\\) refers to the center word embedding while \\(u_{c}\\) to the context word embedding. This is the opposite of what we have seen in CBOW.\n\\[P(w_{j} \\mid w_t) = \\frac{\\exp(u_{j}k_{t})}{\\sum_{w=1}^{V}\\exp(u_{w}k_{t})}\\]\nIf you take the negative log-likelihood for a single context word, given a center:\n\\[ J = -logP(w_{j} \\mid w_t) = -log\\frac{\\exp(u_{j}k_{t})}{\\sum_{w=1}^{V}\\exp(u_{w}k_{t})} = -\\left[logexp(u_{j}k_{t}) - log\\sum_{w=1}^{V}\\exp(u_{w}k_{t})\\right] = -\\left[u_{j}k_{t} -  log\\sum_{w=1}^{V}\\exp(u_{w}k_{t})\\right]  \\]\nExpanding it to all context words:\n\\[ J = -\\sum_{j=1}^{2m}\\left[{u_{j}}{k_t} -log\\sum_{w=1}^{V}\\exp(u_{w}k_{t})\\right]\\]\nBy now, we have grokked the two versions of the Word2Vec algorithms. One detail is still missing though. As it was pointed out, each word has context and one center embedding. How to make one of them? Just simply average them! Next, let’s review some practical limitations and their remedies.\n\n\nExpensive Softmax and Alternatives\nNote that in the softmax normalization, we normalize over the entire vocabulary which is huge. Its comoutational complexity is linear, \\(\\mathcal{O}(n)\\). In the case of Google News dataset it is ca. 6M tokens! Even if you have optimized, vectorized implementation of the softmax calculation, it would take very long to calculate the softmax for each sample in the dataset. As mentioned before, the 2 other innovations of Word2Vec, Hierarchical Softmax and Negative Sampling aim at this computational issue. To limit the scope of this blog post, we will only elaborate on Negative Sampling. While Negative sampling is a more straightforward concept and computationally more efficient, the results of Hierarchical Softmax are not better.\nAt this point I want to point out a little confusion on my part. Though in the first [1] paper, the authors introduced both Skip-Gram and CBOW, in the second they only proceeded with Skip-Gram without any explanation. At least I could not find any. The results in the first paper did not suggest any superiority of Skip-Gram. If you understand this decision, please let me know! :) Anyway, I will proceed in the same way as the authors did and use Skip-Gram accordingly.\n\nNegative Sampling\nThe idea of negative sampling, which was actually introduced in the second Word2Vec paper [2], is simple: instead of contrasting a given center and context word embedding pair against the entire vocabulary, why don’t we just contrast it against noise? In other words, what we aim here is to assess the probability that the center and context words came from the data. The trick is that we ingest negative (or fake if you like) context words so that we expect the model to tell these apart. If I have the center OpenAI and chatGPT as context, I want to assign high probability to this pair but low to a random pair like OpenAI and dachshund or OpenAI and daisy. We use k such negative samples for each positive one. The authors recommend 2-5 negative samples for bigger while 5-20 in smaller datasets. The likelihood function is given as below. \\(w_{t}\\) denotes the center, \\(w_{j}\\) the context word. The tilde marks the negative context sample.\n\\[Likelihood = P(D=1| w_{j}, w_{t}) \\prod_{i=1}^{K}P(D = 0| \\tilde{w_{j_i}}, w_{t}) =  P(D=1) \\prod_{i=1}^{K}(1 - P(D = 1 | \\tilde{w_{j_i}}, w_{t}))\\]\nWord2Vec uses the sigmoid function to model probability and for similarity we again use the dot product. Recall that \\(k_{t}\\) refers to the center word embedding while \\(u_{j}\\) to the context word embedding\n\\[Likelihood = \\frac{1}{1 + exp(-u_{j} k_{t})} \\prod_{i=1}^{K} (1 - \\frac{1}{1 + exp(-u_{j_i} k_{t})}) =  \\frac{1}{1 + exp(-u_{j} k_{t})} \\prod_{i=1}^{K} \\frac{1}{1 + exp(u_{j_i} k_{t})}\\]\nAgain, taking the negative log likelihood (NLL) as traning objective:\n\\[J = - log  \\frac{1}{1 + exp(-u_{j} k_{t})} - \\sum_{i=1}^{K} log \\frac{1}{1 + exp(u_{j_i} k_{t})}\\]\nIt seems that we have a loss function so we are all set! Almost… We still need to figure out how to draw negative samples i.e. define \\(P(\\tilde{w_{j_i}})\\). One might think of using a uniform distribution but that does not take into account word frequencies that we may want to take into consideration. The authors recommend using a scaled version of the unigram distribution \\(U(w)\\) (model based on word frequencies). They found empirically that the unigram distribution raised to the power of 3/4 \\(U(w)^{3/4}\\) works best. To build some intuition why this may be a particularly useful setting, let’s look at the below table of example. We have a 3 words, one is very frequent (at), another less frequent (cinema) and an infrequent one (dachshund). If we raise their relative frequencies to 3/4, then relatively, the infrequent dachshund gets 3x more frequent, cinema 1.7 while at only 1.5. You can see that by using this scaled version of unigram distribution for negative sampling, we increase the probability of drawing less frequent words on the expense of more frequent ones. This intuitively makes sense because without this, we may be drawing too similar negative samples e.g. “the, it” which does not seem to be noise, against we want to contrast the word embeddings.\n\n\n\n\n\n\n\n\nWord\nRelative frequency - \\(f(x)\\)\nRe-scaled relative frequency - \\(f(x)^{3/4}\\)\n\n\n\n\nat\n0.3\n0.45\n\n\ncinema\n0.1\n0.17\n\n\n…\n…\n…\n\n\ndachshund\n0.01\n0.03\n\n\n\n\n\n\nSubsampling of frequent words\nAnother trick that Word2Vec applies to put less emphasis on frequent words is subsampling. The authors argue that words like “in”, “the”, and “a” occur hundreds of millions of times in a large corpus of text. The co-occurence of these words arguably does not provide any useful information with other words e.g. “Paris”. However, the co-occurrence of other words e.g. “Paris” and “France” has immense useful information. Thus, it seems plausible to subsample the frequent words which speeds up training by skipping a share of the frequent word samples. Below, \\(P(x)\\) denotes the probability of x being skippend, \\(f(x)\\) the frequency of the word, \\(t\\) is a chosen threshold recommended to be around \\(10^{-5}\\). It is obvious that infrequent words will have very high probability of being picked. Frequent words are more likely to be skipped but this may be offset by a higher choice of \\(t\\) hyperparameter.\n\\[ P(x) = 1 -  \\sqrt{\\frac{t}{f(x)}}\\]\n\n\nOther notes\nThough this has been quite an in-depth Word2Vec explainer, there is still a lot more to explore within and beyond the scope of Word2vec. I am going to just share a few more things in an unstructured way. You may decide to explore any of these in more detail.\n\nHyperparameters\nThroughout the post, I tried to point out the recommended range of hyperparameters. [8] provides further comments and tips by the authors.\nRecommended HPs: - Embedding size: 50-300 - Context size: 5-10 - Algorithm: Skip-gram/ CBOW - #negative samples: 2-5 (if dataset big enough) if using Negative Sampling and not Hierarchical Softmax - Epochs: 1-3 - Subsampling threshold \\(t\\): ca. \\(10^{-5}\\)\nThe paper does not touch upon what optimizer to use but one might benefit from using an adaptive one like Adam. This might make sense because we may want to apply larger updates to less frequent words.\n\n\nDataset\nThe quality of the dataset is maybe more important than the specific HP settings we choose. The paper used the proprietary Google News dataset but there are now much bigger datasets! Just look at how GloVe [8] benefits from larger and better datasets.\n\n\n\nEvaluation\nEvaluating word vectors is an interesting task. There are 2 ways to do this: 1) intrinsic and 2) extrinsic. In intrinsic evaluation, typically, we do some kind of arithmetic of word embeddings and contrast it to our expectations. There are word similarity and analogy datasets that have been curated by humans. The authors open-sourced their own datasets for intrinsic evaluation: link. It contains semantic and syntactic analogies like “Athens-Greece+Madrid=Spain” and “occasional-occasionally+amazing=amazingly” In extrinsic evaluation, we see how the word embeddings influence the performance of downstream applications where these are used as inputs. Note, that the latter is less sterile because we not only expect “good” word embeddings but also a good interaction between them and the downstream-specific application algorithm.\n\n\nBias\nSince Word2Vec is trained on large corpus of data, it will be exposed of all the biases that surround us. Just think about how more often it will see male references to politicians or engineers and much less for kingergarten teacher or nurse. This begs loads of ethical questions and for solutions. This is a very important topic and I might devote more time to explore it more in a follow-up post. For now, you can take a look at [10] for more details or the “Bias and overgeneralization” part of [11].\n\n\n\nAcknowledgement\nI have made extensive use of the two Word2Vec explainers [1], [5] and the Stanford CS224N notes [3]. I have also used chatGPT to correct my English and generally make the text more comprehensible."
  },
  {
    "objectID": "posts/word2vec/index.html#references",
    "href": "posts/word2vec/index.html#references",
    "title": "Dachshund to Numbers",
    "section": "References",
    "text": "References\n[1] Efficient Estimation of Word Representations inVector Space\n[2] Distributed representations of words and phrases and their compositionality\n[3] word2vec Parameter Learning Explained\n[4] word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method\n[5] Stanford NLP CS224N\n[6] ChatGPT\n[7] Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\n[8] Initial Word2Vec software release\n[9] GloVe: Global Vectors for Word Representation\n[10] Text Embedding Models Contain Bias. Here’s Why That Matters.\n[11] Multimodal neurons in artificial neural networks"
  },
  {
    "objectID": "posts/word2vec/index.html#continous-bag-of-words-cbow",
    "href": "posts/word2vec/index.html#continous-bag-of-words-cbow",
    "title": "Dachs2Num",
    "section": "Continous Bag-of-Words (CBOW)",
    "text": "Continous Bag-of-Words (CBOW)\nIn both CBOW and Skip-Gram, we differentiate between center and context words. The context words around the center word are limited by a context window, which was exactly 4 in the paper. This means that there are 4 context words preceeding (history) and 4 (future) following the center word. The choice of this parameter was not explained in the paper and one might actually want to tune it.\nLet’s look at a specific example but instead of using 4, let’s use a context window of 2 words. Here, the center Word “ChatGPT” is surrounded by the context words “GPT-4”, “outperforms”, “in”, “general”.\n\n\n\n\n\nThe difference between CBOW and Skip-Gram is what we actually want to predict. While CBOW uses the context words to predict the center word, Skip-Gram does the opposite and predicts the context words from the center word.\nBy now, we have a good high-level overview of Word2Vec. It is time to move to the tecnhical implementation of CBOW. We will first look at the trainable weight matrices and an illustrative forward and backward pass. Later, we will interpret the more math-heavy part, the loss function.\n\nIllustrated Forward and Backward Pass\nThere are 2 trainable weight matrices: the input and output matrix. N denotes the number of embedding dimensions, V the vocabulary size. N is usually 50-300 and V in the Google News Dataset, as mentioned before, 3M. The input weight matrix is tasked with generating the input embedding for the task, while the output matrix will produce the predictions i.e. probability distribution over the entire vocabulary. This will make more sense in a bit. For now what is crucial to understand, is that each token has an embedding in the input word matrix, this will be its context embedding and an embedding in the output word matrix which is its center embedding in. In other words, each word has 2 embeddings. The tokens in the Vocabulary need to be mapped to indices, so we can look up their embeddings in the input or output matrices using indexing. FUN FACT: Zyzzyva is an actual word and refers to a type of weevils. I looked up on Google what is the last item in English dictionaries.\n\n\n\n\n\nLet’s put all we know together and a little more by illustrating the forward and backward pass! Below, we use the same example of center and context words. Using the token-index mapping, we look up the indices of the center and context words. The 4 context words are taken from the input matrix and stacked horizontally. To get the average context, we take the row-wise average of the context word embeddings. Using the output (center) matrix, we calculate the dot product between each word in the vocabulary and the averaged context embedding. This is a matrix-vector multiplication. In the resulting score vector each item refers to the similarity (dot product) between the avereage context and the words in the vocabulary. To get a probability distribution over the vocabulary, we apply softmax normalization. The y_true vector will be OHE where the single non-zero element is at the index of the center word. Finally, we apply Cross-Entropy loss and apply backprop to calculate the gradients.\n\n\n\n\n\n\n\nObjective Function Explained\nNow that we have a better understanding of the forward and backward pass of CBOW, let’s look into the math part. Actually, the math of the “vanilla” Word2Vec is not too convoluted, we just need to dissect the objective function. Quick disclaimer: you won’t find these functions in the original paper. Here, I am making use of [2] and [3] but I make an effort at simplyfying the notation and provide a more detailed explanation.\nOur goal is to maximize the data likelihood i.e. to maximize the probability of a center word \\(w_{t-1}\\) conditioned on the context words in context window of c \\(w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c}\\), a given parameter set of \\(\\theta\\) which is the context and center embedding matrices. T denotes the tokens of the dataset i.e. all the words in the dataset.\n\\[\n\\text{Likelihood} = L(\\theta) =  \\prod_{t=1}^{T} P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c}; \\theta)\n\\]\nHow do we calculate \\(P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c})\\)? We are yet to inject the word embeddings into these functions because now they refer to words. In section “Illustrated Forward and Backward Pass”, we said that each word has a corresponding context (input) and center (output) word vector. Let’s denote the context word matrix as \\(K \\in \\mathbb{R}^{n \\times |V|}\\) where \\(k_{i}\\) refers to the i-th column of K meaning the context embedding of word at index i. The center word matrix is denoted by $ U ^{|V| n}$ where center word embeddings are stacked row-wise. Similarly, \\(u_{i}\\) refers to the i-th row of U indicating the center embedding of word at index i.\nLet’s now write up how to express \\(P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c})\\) using the contex and center word embeddings and then explain it in more detail. Below \\(u_{t}\\) refers to the context embedding of the context word at position t. \\(\\bar{k_{c}}\\) refers to the average context embedding of the context words. By taking the dot product between these two, we get a similarity metric (an unnormalized one) between the center word and the average context. The higher the dot-product the more similar the vectors are i.e. the vectors point to a similar direction. By calculating the dot product between all center word embedding and the average context embedding in the vocabulary (the denominator) and applying softmax, we get a probability distribution over the entire vocabulary.\n\\[P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c}) = \\frac{exp(u_{t}\\bar{k_{c}})}{\\sum_{w=1}^{V}exp(u_{w}\\bar{k_{c}})}\\]\nIn machine learning, it is easier to work with the dual problem of maximizing likelihood which is minimizing the negative log likelihood. Using this trick makes the optimization easier. If you want to know why, I recommend you reading this thread. Take the log of the product term results in the summation of logs. By convention, we optimize for the average log-likelihood, hence taking \\(\\frac{1}{T}\\).\n\\[\n\\begin{align*}\nJ(\\theta) & = -\\frac{1}{T} \\log L(\\theta) \\\\\n& = -\\frac{1}{T} \\sum_{t=1}^{T} \\log P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c}; \\theta)\n\\end{align*}\n\\]\nOkay, so now we got a probability distribution over the entire vocabulary which is the prediction of the model but how do we relate this to the ground truth? The cost function does not (yet) include the y_true term though we use it for the cross-entropy loss calculation in the forward pass chart! I just left out this part from the cost function definition to not to make the formula too difficult to understand. The correct loss function for a given sample (a center word and its context) J is:\n\\[J = -\\sum_{w=1}^{V} y_{w}log(\\hat{y}_{w})\\]\nwhere\n\\[\\hat{y} = \\frac{exp(u_{t}\\bar{k_{c}})}{\\sum_{w=1}^{V}exp(u_{w}\\bar{k_{c}})}\\]\nSince the target, y vector is OHE, J reduces to where the i index refers to the target element of the target vector e.g. the index of “ChatGPT” as in the example:\n\\[J = -y_{i}log(\\hat{y_{i}})\\]\nIf you feel like you can extend the formula to contain all training samples of the dataset but this is pretty much all about the CBOW version of Word2Vec except one last thing. In the next section, we will review the training algorithm again and point out its computation complexity and the methods Negative Sampling and Hierarchical Softmas that serve as remedies ot this issue."
  },
  {
    "objectID": "posts/word2vec/index.html#skip-gram",
    "href": "posts/word2vec/index.html#skip-gram",
    "title": "Dachs2Num",
    "section": "Skip-Gram",
    "text": "Skip-Gram\nSkip-Gram flips the role of the context and center words. Here, the center word is used as input to predict the context (output) words. The forward and backward pass are illustrated below in a similar fashion as for CBOW. Observe the differences between the two architectures. Here, the input matrix is the one containing the center word embeddings, the output contains the context ones. We use the center word representation as input to calcute scores for the context outputs. Just like in CBOW, we make a softmax prediction over the entire vocabulary but we contrast these predictions against the multiple target context words which is indicated by the grey box on the right. In this example, we predict the 4 context words from the center.\n\n\n\n\n\nFor ease of notation, only a single sample’s training objective will be detailed. Let’s first start with the likelihood function again. Here, we aim to maximize the probability of context words conditioned on the center word. If we denote context lenght with c, there are 2c context words of which probabilities we aim to maximize (c history and c future context words).\n\\[\\text{Likelihood}=L(\\theta) =\\prod_{j=1}^{2c}P(w_{j} \\mid w_t; \\theta)\\]\nIt is important to note that the formula implies that we make a strong conditional independence assumption. Given a center word, all output words are completely independent (hence you can simply multiply the probabilities). This is definitely not true. Just think of simple grammar. If you have a noun center word followed by a verb context, then the upcoming context is quite unlikely to be a verb again. To give you an example, if you have the word Chelsea (a famous football club and also a district in London) as the center followed be played context then the next context is less likely to be a verb like jump and more likely to be an adjective like marvelously. Seeing this example you may even give semantical arguments why the conditional independence does not hold like nouns more related to football e.g. game is more likely to come up than academics related nouns e.g. conference. Nonetheless, Skip-Gram manages to work well in practice, thus we may be okay with this. By the way, it is the very same assumption that the Naive Bayes Classifier model makes. This naive assumption lets us to break out probabilities so we don’t have to deal with figuring out the joint distributions.\n[NOTE] I think an elaboration of this point may be useful\nThe softmax is used again to get the probability distribution over the vocabulary. Below \\(k_{t}\\) refers to the center word embedding while \\(u_{c}\\) to the context word embedding. This is the opposite of what we have seen in CBOW.\n\\[P(w_{j} \\mid w_t) = \\frac{\\exp(u_{j}k_{t})}{\\sum_{w=1}^{V}\\exp(u_{w}k_{t})}\\]\nIf you take the negative log-likelihood for a single context word, given a center:\n\\[\n\\begin{align*}\nJ & = -\\log P(w_{j} \\mid w_t) \\\\\n& = -\\log\\frac{\\exp(u_{j}k_{t})}{\\sum_{w=1}^{V}\\exp(u_{w}k_{t})} \\\\\n& = -\\left[\\log \\exp(u_{j}k_{t}) - \\log\\sum_{w=1}^{V}\\exp(u_{w}k_{t})\\right] \\\\\n& = -\\left[u_{j}k_{t} -  \\log\\sum_{w=1}^{V}\\exp(u_{w}k_{t})\\right]\n\\end{align*}\n\\]\nExpanding it to all context words:\n\\[ J = -\\sum_{j=1}^{2m}\\left[{u_{j}}{k_t} -log\\sum_{w=1}^{V}\\exp(u_{w}k_{t})\\right]\\]\nBy now, we have grokked the two versions of the Word2Vec algorithms. One detail is still missing though. As it was pointed out, each word has context and one center embedding. How to make one of them? Just simply average them! Next, let’s review some practical limitations and their remedies."
  },
  {
    "objectID": "posts/word2vec/index.html#expensive-softmax-and-alternatives",
    "href": "posts/word2vec/index.html#expensive-softmax-and-alternatives",
    "title": "Dachs2Num",
    "section": "Expensive Softmax and Alternatives",
    "text": "Expensive Softmax and Alternatives\nNote that in the softmax normalization, we normalize over the entire vocabulary which is huge. Its comoutational complexity is linear, \\(\\mathcal{O}(n)\\). In the case of Google News dataset it is ca. 6M tokens! Even if you have optimized, vectorized implementation of the softmax calculation, it would take very long to calculate the softmax for each sample in the dataset. As mentioned before, the 2 other innovations of Word2Vec, Hierarchical Softmax and Negative Sampling aim at this computational issue. To limit the scope of this blog post, we will only elaborate on Negative Sampling. While Negative sampling is a more straightforward concept and computationally more efficient, the results of Hierarchical Softmax are not better.\nAt this point I want to point out a little confusion on my part. Though in the first [1] paper, the authors introduced both Skip-Gram and CBOW, in the second they only proceeded with Skip-Gram without any explanation. At least I could not find any. The results in the first paper did not suggest any superiority of Skip-Gram. If you understand this decision, please let me know! :) Anyway, I will proceed in the same way as the authors did and use Skip-Gram accordingly.\n\nNegative Sampling\nThe idea of negative sampling, which was actually introduced in the second Word2Vec paper [2], is simple: instead of contrasting a given center and context word embedding pair against the entire vocabulary, why don’t we just contrast it against noise? In other words, what we aim here is to assess the probability that the center and context words came from the data. The trick is that we ingest negative (or fake if you like) context words so that we expect the model to tell these apart. If I have the center OpenAI and chatGPT as context, I want to assign high probability to this pair but low to a random pair like OpenAI and dachshund or OpenAI and daisy. We use k such negative samples for each positive one. The authors recommend 2-5 negative samples for bigger while 5-20 in smaller datasets. The likelihood function is given as below. \\(w_{t}\\) denotes the center, \\(w_{j}\\) the context word. The tilde marks the negative context sample.\n\\[\n\\begin{align*}\n\\text{Likelihood} & = P(D=1| w_{j}, w_{t}) \\prod_{i=1}^{K}P(D = 0| \\tilde{w_{j_i}}, w_{t}) \\\\\n& =  P(D=1) \\prod_{i=1}^{K}(1 - P(D = 1 | \\tilde{w_{j_i}}, w_{t}))\n\\end{align*}\n\\]\nWord2Vec uses the sigmoid function to model probability and for similarity we again use the dot product. Recall that \\(k_{t}\\) refers to the center word embedding while \\(u_{j}\\) to the context word embedding \\[\n\\begin{align*}\n\\text{Likelihood} & = \\frac{1}{1 + \\exp(-u_{j} k_{t})} \\prod_{i=1}^{K} \\left(1 - \\frac{1}{1 + \\exp(-u_{j_i} k_{t})}\\right) \\\\\n& =  \\frac{1}{1 + \\exp(-u_{j} k_{t})} \\prod_{i=1}^{K} \\frac{1}{1 + \\exp(u_{j_i} k_{t})}\n\\end{align*}\n\\]\nAgain, taking the negative log likelihood (NLL) as traning objective:\n\\[J = - log  \\frac{1}{1 + exp(-u_{j} k_{t})} - \\sum_{i=1}^{K} log \\frac{1}{1 + exp(u_{j_i} k_{t})}\\]\nIt seems that we have a loss function so we are all set! Almost… We still need to figure out how to draw negative samples i.e. define \\(P(\\tilde{w_{j_i}})\\). One might think of using a uniform distribution but that does not take into account word frequencies that we may want to take into consideration. The authors recommend using a scaled version of the unigram distribution \\(U(w)\\) (model based on word frequencies). They found empirically that the unigram distribution raised to the power of 3/4 \\(U(w)^{3/4}\\) works best. To build some intuition why this may be a particularly useful setting, let’s look at the below table of example. We have a 3 words, one is very frequent (at), another less frequent (cinema) and an infrequent one (dachshund). If we raise their relative frequencies to 3/4, then relatively, the infrequent dachshund gets 3x more frequent, cinema 1.7 while at only 1.5. You can see that by using this scaled version of unigram distribution for negative sampling, we increase the probability of drawing less frequent words on the expense of more frequent ones. This intuitively makes sense because without this, we may be drawing too similar negative samples e.g. “the, it” which does not seem to be noise, against we want to contrast the word embeddings.\n\n\n\n\n\n\n\n\nWord\nRelative frequency - \\(f(x)\\)\nRe-scaled relative frequency - \\(f(x)^{3/4}\\)\n\n\n\n\nat\n0.3\n0.45\n\n\ncinema\n0.1\n0.17\n\n\n…\n…\n…\n\n\ndachshund\n0.01\n0.03"
  },
  {
    "objectID": "posts/word2vec/index.html#subsampling-of-frequent-words",
    "href": "posts/word2vec/index.html#subsampling-of-frequent-words",
    "title": "Dachs2Num",
    "section": "Subsampling of frequent words",
    "text": "Subsampling of frequent words\nAnother trick that Word2Vec applies to put less emphasis on frequent words is subsampling. The authors argue that words like “in”, “the”, and “a” occur hundreds of millions of times in a large corpus of text. The co-occurence of these words arguably does not provide any useful information with other words e.g. “Paris”. However, the co-occurrence of other words e.g. “Paris” and “France” has immense useful information. Thus, it seems plausible to subsample the frequent words which speeds up training by skipping a share of the frequent word samples. Below, \\(P(x)\\) denotes the probability of x being skippend, \\(f(x)\\) the frequency of the word, \\(t\\) is a chosen threshold recommended to be around \\(10^{-5}\\). It is obvious that infrequent words will have very high probability of being picked. Frequent words are more likely to be skipped but this may be offset by a higher choice of \\(t\\) hyperparameter.\n\\[ P(x) = 1 -  \\sqrt{\\frac{t}{f(x)}}\\]"
  },
  {
    "objectID": "posts/word2vec/index.html#other-notes",
    "href": "posts/word2vec/index.html#other-notes",
    "title": "Dachs2Num",
    "section": "Other notes",
    "text": "Other notes\nThough this has been quite an in-depth Word2Vec explainer, there is still a lot more to explore within and beyond the scope of Word2vec. I am going to just share a few more things in an unstructured way. You may decide to explore any of these in more detail.\n\nHyperparameters\nThroughout the post, I tried to point out the recommended range of hyperparameters. [8] provides further comments and tips by the authors.\nRecommended HPs: - Embedding size: 50-300 - Context size: 5-10 - Algorithm: Skip-gram/ CBOW - #negative samples: 2-5 (if dataset big enough) if using Negative Sampling and not Hierarchical Softmax - Epochs: 1-3 - Subsampling threshold \\(t\\): ca. \\(10^{-5}\\)\nThe paper does not touch upon what optimizer to use but one might benefit from using an adaptive one like Adam. This might make sense because we may want to apply larger updates to less frequent words.\n\n\nDataset\nThe quality of the dataset is maybe more important than the specific HP settings we choose. The paper used the proprietary Google News dataset but there are now much bigger datasets! Just look at how GloVe [8] benefits from larger and better datasets.\n\n\n\n\n\n\n\nEvaluation\nEvaluating word vectors is an interesting task. There are 2 ways to do this: 1) intrinsic and 2) extrinsic. In intrinsic evaluation, typically, we do some kind of arithmetic of word embeddings and contrast it to our expectations. There are word similarity and analogy datasets that have been curated by humans. The authors open-sourced their own datasets for intrinsic evaluation: link. It contains semantic and syntactic analogies like “Athens-Greece+Madrid=Spain” and “occasional-occasionally+amazing=amazingly” In extrinsic evaluation, we see how the word embeddings influence the performance of downstream applications where these are used as inputs. Note, that the latter is less sterile because we not only expect “good” word embeddings but also a good interaction between them and the downstream-specific application algorithm.\n\n\nBias\nSince Word2Vec is trained on large corpus of data, it will be exposed of all the biases that surround us. Just think about how more often it will see male references to politicians or engineers and much less for kingergarten teacher or nurse. This begs loads of ethical questions and for solutions. This is a very important topic and I might devote more time to explore it more in a follow-up post. For now, you can take a look at [10] for more details or the “Bias and overgeneralization” part of [11]."
  },
  {
    "objectID": "posts/word2vec/index.html#acknowledgement",
    "href": "posts/word2vec/index.html#acknowledgement",
    "title": "Dachs2Num",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nI have made extensive use of the two Word2Vec explainers [1], [5] and the Stanford CS224N notes [3]. I have also used chatGPT to correct my English and generally make the text more comprehensible."
  },
  {
    "objectID": "posts/word2vec/reworked_Word2Vec.html",
    "href": "posts/word2vec/reworked_Word2Vec.html",
    "title": "Word2Vec Explained",
    "section": "",
    "text": "Word2Vec is one of the most well-known word embedding algorithms. Although it has been succeeded in efficacy by more recent transformer-based algorithms like BERT and GPT, Word2Vec remains a valuable algorithm to understand. It was the first algorithm to produce word embeddings that captured both syntactic and semantic relationships between words, famously demonstrated by the “king - man + woman = queen” analogy. This made a profound contribution to natural language processing and kicked off a swath of impressive developments. In this post, we will take a deep dive into the nuts and bolts of Word2Vec. But before we do that, let’s first review why word embeddings are crucial for NLP tasks.\n\n\n\nNLP Chronology"
  },
  {
    "objectID": "posts/word2vec/reworked_Word2Vec.html#intro",
    "href": "posts/word2vec/reworked_Word2Vec.html#intro",
    "title": "Word2Vec Explained",
    "section": "",
    "text": "Word2Vec is one of the most well-known word embedding algorithms. Although it has been succeeded in efficacy by more recent transformer-based algorithms like BERT and GPT, Word2Vec remains a valuable algorithm to understand. It was the first algorithm to produce word embeddings that captured both syntactic and semantic relationships between words, famously demonstrated by the “king - man + woman = queen” analogy. This made a profound contribution to natural language processing and kicked off a swath of impressive developments. In this post, we will take a deep dive into the nuts and bolts of Word2Vec. But before we do that, let’s first review why word embeddings are crucial for NLP tasks.\n\n\n\nNLP Chronology"
  },
  {
    "objectID": "posts/word2vec/reworked_Word2Vec.html#word-embeddings",
    "href": "posts/word2vec/reworked_Word2Vec.html#word-embeddings",
    "title": "Word2Vec Explained",
    "section": "Word embeddings",
    "text": "Word embeddings\n[NOTE This section could definitely be more elaborated from a theoretical point. Also, better illustration and more examples could help.]\nWord embeddings (also called as vectors, representations) are high-dimensional (usually 50-1000 dimensions) dense vectors over which you can do all kinds of arithmetics and most importantly encode similarity. Arithmetics like the notories example “king–man+woman=queen” or “good-best+bad=worst”. These two examples illustrate how semantic (meaning) and syntactic (grammar) similarities are encoded in the word vectors.\nThe whole idea of word embeddings is based on distributional semantics which is the concept of representing a word based on its usual context. This idea waw popularized by a linguist, J. R. Firth, with his famous quote “You shall know a word by the company it keeps” in the 1950s. “The basic idea of distributional semantics can be summed up in the so-called distributional hypothesis: linguistic items with similar distributions have similar meanings.”\n\n\n\nKing_dachs\n\n\n[ NOTE ] I WOULD EXCLUDE THIS\nAs a non-linguist, I can certainly say this makes a lots of sense based on my own experience when reading in English as a non-native speaker. I asked ChatGPT to provide me a few English phrases, that a non-native speaker is likely to be unaware of, with example sentences. The word “sesquipedalian” is definitely new to me but I could guess it means something like a formal, hard-to-follow, academic language after reading the word in context: ““Although he was a brilliant scholar, his lectures were often difficult to follow due to his tendency towards sesquipedalian language.”\nWhat is a reasonable alternative to dense word representations? One-hot-encoding (OHE) might first come to your mind which was the way to go in traditional NLP. When using OHE, you basically treat words as discrete, atomic symbols. This way of representing words is usually refered to as sparse, in contrast to dense, distributed representation (like Word2Vec).\nLet’s look at a toy example to illustrate the differences better. Let’s say I want to represent the words: “Chinese crested dog”, “Dachshund”, “Bolognese”, “Labrador” and “Newfoondland”. Using sparse, OHE representation, we would end up with the below word vectors. The individual words have their own dedicated dimension and if you were to calculate similarity between of these words vectors e.g. dot product, you end up with 0.\n\n\n\nOHE\n\n\nLet’s see how we could represent these words in a dense way. How about we define 2 dimensions that are suitable to capture the meaning of these words to some extent. I think the body size (y-axis) and the fur size(x-axis) are good candidates for our goal. Below you can see my estimation of the word vectors.\n\n\n\nDense\n\n\nHere is how the word vectors compare to each other. For the dense representation, distributed, representation only 2 dimensions are used in contrast to the 5 dimensions using sparse, OHE representation. In my opinion, this toy example illustrates how dense vectors capture meaning. Using the very simple distance metric, the taxicab distance (also known as L1 and Manhattan distance), we can clearly see that a dachshund is more similar to bolognese (d=0.88) than to a Newfoondland (d=4.78) because the distance between the two is smaller. Using sparse representation, you would be not able to calculate different degrees of similarity. All sparse vectors would have 0 similarity because the vectors are orthogonal to each other.\n\n\n\ndense-vs-sparse-dog\n\n\nNOTE: though in this toy example, the dimensions of the dense vectors have a direct interpretation (body and fur size), this is not the case in a real application. There are papers attempting to interpret word vector dimensions in one way or another. For example, in [7] they attempted to extract the ‘gender’ dimension (and debias the word vectors, more about this in section ‘Other Notes’)\nBack to the question of why we care about (good) word embeddings. We could not only decrease the dimensionality of the representation which may have its own regularization effect but we did in a way that we can now compare them and calculate similarities. This way of encoding information seems a lot mor informative than the sparse one! Intuitively, it just makes sense that any downstream application (chatbots, named entity recognition, text summarization) would prefer making use of this information. Isn’t this fascinating? We have assessed the importance of word vectors, now let’s say how to make one!"
  },
  {
    "objectID": "posts/word2vec/reworked_Word2Vec.html#the-word2vec-algorithm",
    "href": "posts/word2vec/reworked_Word2Vec.html#the-word2vec-algorithm",
    "title": "Word2Vec Explained",
    "section": "The Word2Vec algorithm",
    "text": "The Word2Vec algorithm\nHere, better and more illustrations would be welcome. Some ideas in particular: sliding over the text, show dot products for a given context size with examples for both CBOW and Skip-Gram\nIn summary, Word2Vec beat all previous benchmarks and managed to cut down the on training time vastly which allowed the authors to train on a larger corpus than it was possible before. They used the Google News, a Google proprietary, dataset which contains ca. 6B tokens and has a vocabulary size of 3M. For reference, the Oxford English Dictionary contains over 600,000 words and Phrases [4]. What comes to my surprise is that only a few epochs are needed to train the model. In the paper they used 1-3 epochs. Probably this is enough because the 6B token long dataset containts ca. 6B samples which is quite a number. FUN FACT: Word2Vec was originally implemented on CPUs. It was 2013 and GPUs were already on their way revolutionizing AI but still a year after the seminal paper of Krizhevsky et. al. I guess there were strong reasons why they did all this on CPUs. At this point, I think it is important to note that Word2Vec was first introduced in [1] and then further developed in [2].\nIt is now time to give a one sentence description of Word2Vec:\n\nWord2Vec is a neural network-based pre-training, unsupervised algorithm that produces word embeddings, trained on a large corpus of text data which can be used as input features for various natural language processing tasks.\n\nThis is quite a dense description which is worth elaborating in more detail. It is important to highlight that Word2Vec is an NN-based algorithm because around the time of its publication there were other, non-NN-based algorithms like count-based methods e.g. co-occurence matrix factorization methods and GloVe [9]. It is a pre-training algorithm because its resulting word embeddings is mostly used for downstream NLP tasks where these embeddings are either frozen or jointly optimzied in the downstream task. It is also unsupervised which means that no human supervision like labels are being used during training. The supervision comes from the data itself i.e. how words follow each other.\nThe success of Word2Vec was based on multiple innovations: 1. Two new training algorithms: - Continous Bag-Of-Word (CBOW) - Skip-Gram 1. Two tricks optimizing computational efficiency - Negative Sampling - Hierarchical Softmax\nHere, I am using a somewhat sloppy description because strictly speaking Negative Sampling and Hierarchical Softmax are also part of the training algorithm. I just refer to them as “computational tricks” because, to me, they do not really deal with high-level concepts like how to relate context to a specific (center) word but more about the nuanced implementation details of the loss function. I think later this taxonomy will make sense.\nFirst, let’s review the easier of the two training algorithms, CBOW.\n\nContinous Bag-of-Words (CBOW)\nIn both CBOW and Skip-Gram, we differentiate between center and context words. The context words around the center word are limited by a context window, which was exactly 4 in the paper. This means that there are 4 context words preceeding (history) and 4 (future) following the center word. The choice of this parameter was not explained in the paper and one might actually want to tune it.\nLet’s look at a specific example but instead of using 4, let’s use a context window of 2 words. Here, the center Word “ChatGPT” is surrounded by the context words “GPT-4”, “outperforms”, “in”, “general”.\n\n\n\nEx-Sentence\n\n\nThe difference between CBOW and Skip-Gram is what we actually want to predict. While CBOW uses the context words to predict the center word, Skip-Gram does the opposite and predicts the context words from the center word.\nBy now, we have a good high-level overview of Word2Vec. It is time to move to the tecnhical implementation of CBOW. We will first look at the trainable weight matrices and an illustrative forward and backward pass. Later, we will interpret the more math-heavy part, the loss function.\n\nIllustrated Forward and Backward Pass\nThere are 2 trainable weight matrices: the input and output matrix. N denotes the number of embedding dimensions, V the vocabulary size. N is usually 50-300 and V in the Google News Dataset, as mentioned before, 3M. The input weight matrix is tasked with generating the input embedding for the task, while the output matrix will produce the predictions i.e. probability distribution over the entire vocabulary. This will make more sense in a bit. For now what is crucial to understand, is that each token has an embedding in the input word matrix, this will be its context embedding and an embedding in the output word matrix which is its center embedding in. In other words, each word has 2 embeddings. The tokens in the Vocabulary need to be mapped to indices, so we can look up their embeddings in the input or output matrices using indexing. FUN FACT: Zyzzyva is an actual word and refers to a type of weevils. I looked up on Google what is the last item in English dictionaries.\n\nLet’s put all we know together and a little more by illustrating the forward and backward pass! Below, we use the same example of center and context words. Using the token-index mapping, we look up the indices of the center and context words. The 4 context words are taken from the input matrix and stacked horizontally. To get the average context, we take the row-wise average of the context word embeddings. Using the output (center) matrix, we calculate the dot product between each word in the vocabulary and the averaged context embedding. This is a matrix-vector multiplication. In the resulting score vector each item refers to the similarity (dot product) between the avereage context and the words in the vocabulary. To get a probability distribution over the vocabulary, we apply softmax normalization. The y_true vector will be OHE where the single non-zero element is at the index of the center word. Finally, we apply Cross-Entropy loss and apply backprop to calculate the gradients.\n\n\n\nObjective Function Explained\nNow that we have a better understanding of the forward and backward pass of CBOW, let’s look into the math part. Actually, the math of the “vanilla” Word2Vec is not too convoluted, we just need to dissect the objective function. Quick disclaimer: you won’t find these functions in the original paper. Here, I am making use of [2] and [3] but I make an effort at simplyfying the notation and provide a more detailed explanation.\nOur goal is to maximize the data likelihood i.e. to maximize the probability of a center word \\(w_{t-1}\\) conditioned on the context words in context window of c \\(w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c}\\), a given parameter set of \\(\\theta\\) which is the context and center embedding matrices. T denotes the tokens of the dataset i.e. all the words in the dataset.\n\\[ Likelihood = L(\\theta) =  \\prod_{t=1}^{T} P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c}; \\theta) \\]\nHow do we calculate \\(P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c})\\)? We are yet to inject the word embeddings into these functions because now they refer to words. In section “Illustrated Forward and Backward Pass”, we said that each word has a corresponding context (input) and center (output) word vector. Let’s denote the context word matrix as \\(K \\in \\mathbb{R}^{n \\times |V|}\\) where \\(k_{i}\\) refers to the i-th column of K meaning the context embedding of word at index i. The center word matrix is denoted by $ U ^{|V| n}$ where center word embeddings are stacked row-wise. Similarly, \\(u_{i}\\) refers to the i-th row of U indicating the center embedding of word at index i.\nLet’s now write up how to express \\(P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c})\\) using the contex and center word embeddings and then explain it in more detail. Below \\(u_{t}\\) refers to the context embedding of the context word at position t. \\(\\bar{k_{c}}\\) refers to the average context embedding of the context words. By taking the dot product between these two, we get a similarity metric (an unnormalized one) between the center word and the average context. The higher the dot-product the more similar the vectors are i.e. the vectors point to a similar direction. By calculating the dot product between all center word embedding and the average context embedding in the vocabulary (the denominator) and applying softmax, we get a probability distribution over the entire vocabulary.\n\\[P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c}) = \\frac{exp(u_{t}\\bar{k_{c}})}{\\sum_{w=1}^{V}exp(u_{w}\\bar{k_{c}})}\\]\nIn machine learning, it is easier to work with the dual problem of maximizing likelihood which is minimizing the negative log likelihood. Using this trick makes the optimization easier. If you want to know why, I recommend you reading this thread. Take the log of the product term results in the summation of logs. By convention, we optimize for the average log-likelihood, hence taking \\(\\frac{1}{T}\\).\n\\[ J(\\theta) = -\\frac{1}{T} \\log L(\\theta) = -\\frac{1}{T} \\sum_{t=1}^{T} \\log P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c}; \\theta) \\]\nOkay, so now we got a probability distribution over the entire vocabulary which is the prediction of the model but how do we relate this to the ground truth? The cost function does not (yet) include the y_true term though we use it for the cross-entropy loss calculation in the forward pass chart! I just left out this part from the cost function definition to not to make the formula too difficult to understand. The correct loss function for a given sample (a center word and its context) J is:\n\\[J = -\\sum_{w=1}^{V} y_{w}log(\\hat{y}_{w})\\]\nwhere\n\\[\\hat{y} = \\frac{exp(u_{t}\\bar{k_{c}})}{\\sum_{w=1}^{V}exp(u_{w}\\bar{k_{c}})}\\]\nSince the target, y vector is OHE, J reduces to where the i index refers to the target element of the target vector e.g. the index of “ChatGPT” as in the example:\n\\[J = -y_{i}log(\\hat{y_{i}})\\]\nIf you feel like you can extend the formula to contain all training samples of the dataset but this is pretty much all about the CBOW version of Word2Vec except one last thing. In the next section, we will review the training algorithm again and point out its computation complexity and the methods Negative Sampling and Hierarchical Softmas that serve as remedies ot this issue.\n\n\n\nSkip-Gram\nSkip-Gram flips the role of the context and center words. Here, the center word is used as input to predict the context (output) words. The forward and backward pass are illustrated below in a similar fashion as for CBOW. Observe the differences between the two architectures. Here, the input matrix is the one containing the center word embeddings, the output contains the context ones. We use the center word representation as input to calcute scores for the context outputs. Just like in CBOW, we make a softmax prediction over the entire vocabulary but we contrast these predictions against the multiple target context words which is indicated by the grey box on the right. In this example, we predict the 4 context words from the center.\n\n\n\nSkip-Gram\n\n\nFor ease of notation, only a single sample’s training objective will be detailed. Let’s first start with the likelihood function again. Here, we aim to maximize the probability of context words conditioned on the center word. If we denote context lenght with c, there are 2c context words of which probabilities we aim to maximize (c history and c future context words).\n\\[Likelihood=L(\\theta) =\\prod_{j=1}^{2c}P(w_{j} \\mid w_t; \\theta)\\]\nIt is important to note that the formula implies that we make a strong conditional independence assumption. Given a center word, all output words are completely independent (hence you can simply multiply the probabilities). This is definitely not true. Just think of simple grammar. If you have a noun center word followed by a verb context, then the upcoming context is quite unlikely to be a verb again. To give you an example, if you have the word Chelsea (a famous football club and also a district in London) as the center followed be played context then the next context is less likely to be a verb like jump and more likely to be an adjective like marvelously. Seeing this example you may even give semantical arguments why the conditional independence does not hold like nouns more related to football e.g. game is more likely to come up than academics related nouns e.g. conference. Nonetheless, Skip-Gram manages to work well in practice, thus we may be okay with this. By the way, it is the very same assumption that the Naive Bayes Classifier model makes. This naive assumption lets us to break out probabilities so we don’t have to deal with figuring out the joint distributions.\n[NOTE] I think an elaboration of this point may be useful\nThe softmax is used again to get the probability distribution over the vocabulary. Below \\(k_{t}\\) refers to the center word embedding while \\(u_{c}\\) to the context word embedding. This is the opposite of what we have seen in CBOW.\n\\[P(w_{j} \\mid w_t) = \\frac{\\exp(u_{j}k_{t})}{\\sum_{w=1}^{V}\\exp(u_{w}k_{t})}\\]\nIf you take the negative log-likelihood for a single context word, given a center:\n\\[ J = -logP(w_{j} \\mid w_t) = -log\\frac{\\exp(u_{j}k_{t})}{\\sum_{w=1}^{V}\\exp(u_{w}k_{t})} = -\\left[logexp(u_{j}k_{t}) - log\\sum_{w=1}^{V}\\exp(u_{w}k_{t})\\right] = -\\left[u_{j}k_{t} -  log\\sum_{w=1}^{V}\\exp(u_{w}k_{t})\\right]  \\]\nExpanding it to all context words:\n\\[ J = -\\sum_{j=1}^{2m}\\left[{u_{j}}{k_t} -log\\sum_{w=1}^{V}\\exp(u_{w}k_{t})\\right]\\]\nBy now, we have grokked the two versions of the Word2Vec algorithms. One detail is still missing though. As it was pointed out, each word has context and one center embedding. How to make one of them? Just simply average them! Next, let’s review some practical limitations and their remedies.\n\n\nExpensive Softmax and Alternatives\nNote that in the softmax normalization, we normalize over the entire vocabulary which is huge. Its comoutational complexity is linear, \\(\\mathcal{O}(n)\\). In the case of Google News dataset it is ca. 6M tokens! Even if you have optimized, vectorized implementation of the softmax calculation, it would take very long to calculate the softmax for each sample in the dataset. As mentioned before, the 2 other innovations of Word2Vec, Hierarchical Softmax and Negative Sampling aim at this computational issue. To limit the scope of this blog post, we will only elaborate on Negative Sampling. While Negative sampling is a more straightforward concept and computationally more efficient, the results of Hierarchical Softmax are not better.\nAt this point I want to point out a little confusion on my part. Though in the first [1] paper, the authors introduced both Skip-Gram and CBOW, in the second they only proceeded with Skip-Gram without any explanation. At least I could not find any. The results in the first paper did not suggest any superiority of Skip-Gram. If you understand this decision, please let me know! :) Anyway, I will proceed in the same way as the authors did and use Skip-Gram accordingly.\n\nNegative Sampling\nThe idea of negative sampling, which was actually introduced in the second Word2Vec paper [2], is simple: instead of contrasting a given center and context word embedding pair against the entire vocabulary, why don’t we just contrast it against noise? In other words, what we aim here is to assess the probability that the center and context words came from the data. The trick is that we ingest negative (or fake if you like) context words so that we expect the model to tell these apart. If I have the center OpenAI and chatGPT as context, I want to assign high probability to this pair but low to a random pair like OpenAI and dachshund or OpenAI and daisy. We use k such negative samples for each positive one. The authors recommend 2-5 negative samples for bigger while 5-20 in smaller datasets. The likelihood function is given as below. \\(w_{t}\\) denotes the center, \\(w_{j}\\) the context word. The tilde marks the negative context sample.\n\\[Likelihood = P(D=1| w_{j}, w_{t}) \\prod_{i=1}^{K}P(D = 0| \\tilde{w_{j_i}}, w_{t}) =  P(D=1) \\prod_{i=1}^{K}(1 - P(D = 1 | \\tilde{w_{j_i}}, w_{t}))\\]\nWord2Vec uses the sigmoid function to model probability and for similarity we again use the dot product. Recall that \\(k_{t}\\) refers to the center word embedding while \\(u_{j}\\) to the context word embedding\n\\[Likelihood = \\frac{1}{1 + exp(-u_{j} k_{t})} \\prod_{i=1}^{K} (1 - \\frac{1}{1 + exp(-u_{j_i} k_{t})}) =  \\frac{1}{1 + exp(-u_{j} k_{t})} \\prod_{i=1}^{K} \\frac{1}{1 + exp(u_{j_i} k_{t})}\\]\nAgain, taking the negative log likelihood (NLL) as traning objective:\n\\[J = - log  \\frac{1}{1 + exp(-u_{j} k_{t})} - \\sum_{i=1}^{K} log \\frac{1}{1 + exp(u_{j_i} k_{t})}\\]\nIt seems that we have a loss function so we are all set! Almost… We still need to figure out how to draw negative samples i.e. define \\(P(\\tilde{w_{j_i}})\\). One might think of using a uniform distribution but that does not take into account word frequencies that we may want to take into consideration. The authors recommend using a scaled version of the unigram distribution \\(U(w)\\) (model based on word frequencies). They found empirically that the unigram distribution raised to the power of 3/4 \\(U(w)^{3/4}\\) works best. To build some intuition why this may be a particularly useful setting, let’s look at the below table of example. We have a 3 words, one is very frequent (at), another less frequent (cinema) and an infrequent one (dachshund). If we raise their relative frequencies to 3/4, then relatively, the infrequent dachshund gets 3x more frequent, cinema 1.7 while at only 1.5. You can see that by using this scaled version of unigram distribution for negative sampling, we increase the probability of drawing less frequent words on the expense of more frequent ones. This intuitively makes sense because without this, we may be drawing too similar negative samples e.g. “the, it” which does not seem to be noise, against we want to contrast the word embeddings.\n\n\n\n\n\n\n\n\nWord\nRelative frequency - \\(f(x)\\)\nRe-scaled relative frequency - \\(f(x)^{3/4}\\)\n\n\n\n\nat\n0.3\n0.45\n\n\ncinema\n0.1\n0.17\n\n\n…\n…\n…\n\n\ndachshund\n0.01\n0.03\n\n\n\n\n\n\nSubsampling of frequent words\nAnother trick that Word2Vec applies to put less emphasis on frequent words is subsampling. The authors argue that words like “in”, “the”, and “a” occur hundreds of millions of times in a large corpus of text. The co-occurence of these words arguably does not provide any useful information with other words e.g. “Paris”. However, the co-occurrence of other words e.g. “Paris” and “France” has immense useful information. Thus, it seems plausible to subsample the frequent words which speeds up training by skipping a share of the frequent word samples. Below, \\(P(x)\\) denotes the probability of x being skippend, \\(f(x)\\) the frequency of the word, \\(t\\) is a chosen threshold recommended to be around \\(10^{-5}\\). It is obvious that infrequent words will have very high probability of being picked. Frequent words are more likely to be skipped but this may be offset by a higher choice of \\(t\\) hyperparameter.\n\\[ P(x) = 1 -  \\sqrt{\\frac{t}{f(x)}}\\]\n\n\nOther notes\nThough this has been quite an in-depth Word2Vec explainer, there is still a lot more to explore within and beyond the scope of Word2vec. I am going to just share a few more things in an unstructured way. You may decide to explore any of these in more detail.\n\nHyperparameters\nThroughout the post, I tried to point out the recommended range of hyperparameters. [8] provides further comments and tips by the authors.\nRecommended HPs: - Embedding size: 50-300 - Context size: 5-10 - Algorithm: Skip-gram/ CBOW - #negative samples: 2-5 (if dataset big enough) if using Negative Sampling and not Hierarchical Softmax - Epochs: 1-3 - Subsampling threshold \\(t\\): ca. \\(10^{-5}\\)\nThe paper does not touch upon what optimizer to use but one might benefit from using an adaptive one like Adam. This might make sense because we may want to apply larger updates to less frequent words.\n\n\nDataset\nThe quality of the dataset is maybe more important than the specific HP settings we choose. The paper used the proprietary Google News dataset but there are now much bigger datasets! Just look at how GloVe [8] benefits from larger and better datasets.\n\n\n\nEvaluation\nEvaluating word vectors is an interesting task. There are 2 ways to do this: 1) intrinsic and 2) extrinsic. In intrinsic evaluation, typically, we do some kind of arithmetic of word embeddings and contrast it to our expectations. There are word similarity and analogy datasets that have been curated by humans. The authors open-sourced their own datasets for intrinsic evaluation: link. It contains semantic and syntactic analogies like “Athens-Greece+Madrid=Spain” and “occasional-occasionally+amazing=amazingly” In extrinsic evaluation, we see how the word embeddings influence the performance of downstream applications where these are used as inputs. Note, that the latter is less sterile because we not only expect “good” word embeddings but also a good interaction between them and the downstream-specific application algorithm.\n\n\nBias\nSince Word2Vec is trained on large corpus of data, it will be exposed of all the biases that surround us. Just think about how more often it will see male references to politicians or engineers and much less for kingergarten teacher or nurse. This begs loads of ethical questions and for solutions. This is a very important topic and I might devote more time to explore it more in a follow-up post. For now, you can take a look at [10] for more details or the “Bias and overgeneralization” part of [11].\n\n\n\nAcknowledgement\nI have made extensive use of the two Word2Vec explainers [1], [5] and the Stanford CS224N notes [3]. I have also used chatGPT to correct my English and generally make the text more comprehensible."
  },
  {
    "objectID": "posts/word2vec/reworked_Word2Vec.html#references",
    "href": "posts/word2vec/reworked_Word2Vec.html#references",
    "title": "Word2Vec Explained",
    "section": "References",
    "text": "References\n[1] Efficient Estimation of Word Representations inVector Space\n[2] Distributed representations of words and phrases and their compositionality\n[3] word2vec Parameter Learning Explained\n[4] word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method\n[5] Stanford NLP CS224N\n[6] ChatGPT\n[7] Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\n[8] Initial Word2Vec software release\n[9] GloVe: Global Vectors for Word Representation\n[10] Text Embedding Models Contain Bias. Here’s Why That Matters.\n[11] Multimodal neurons in artificial neural networks"
  }
]
[
  {
    "objectID": "posts/word2vec/index.html",
    "href": "posts/word2vec/index.html",
    "title": "Dachs2Num",
    "section": "",
    "text": "Word2Vec is one of the most well-known word embedding algorithms. Although it has been succeeded in efficacy by more recent transformer-based algorithms like BERT and GPT, Word2Vec remains a valuable algorithm to understand. It was the first algorithm to produce word embeddings that captured both syntactic and semantic relationships between words, famously demonstrated by the “king - man + woman = queen” analogy. This made a profound contribution to natural language processing and kicked off a swath of impressive developments. In this post, we will take a deep dive into the nuts and bolts of Word2Vec. But before we do that, let’s first review why word embeddings are crucial for NLP tasks."
  },
  {
    "objectID": "posts/word2vec/index.html#continous-bag-of-words-cbow",
    "href": "posts/word2vec/index.html#continous-bag-of-words-cbow",
    "title": "Dachs2Num",
    "section": "Continous Bag-of-Words (CBOW)",
    "text": "Continous Bag-of-Words (CBOW)\nIn both CBOW and Skip-Gram, we differentiate between center and context words. The context words around the center word are limited by a context window, which was exactly 4 in the paper. This means that there are 4 context words preceeding (history) and 4 (future) following the center word. The choice of this parameter was not explained in the paper and one might actually want to tune it.\nLet’s look at a specific example but instead of using 4, let’s use a context window of 2 words. Here, the center Word “ChatGPT” is surrounded by the context words “GPT-4”, “outperforms”, “in”, “general”.\n\n\n\nThe difference between CBOW and Skip-Gram is what we actually want to predict. While CBOW uses the context words to predict the center word, Skip-Gram does the opposite and predicts the context words from the center word.\n\n\n\nBy now, we have a good high-level overview of Word2Vec. It is time to move to the technical implementation of CBOW. We will first look at the trainable weight matrices and an illustrative forward and backward pass. Later, we will interpret the more math-heavy part, the loss function.\n\nIllustrated Forward and Backward Pass\nThere are 2 trainable weight matrices: the input and output matrix.\n\nN denotes the number of embedding dimensions,\nV the vocabulary size.\n\nN is usually 50-300 and V in the Google News Dataset, as mentioned before, 3M.\nThe input weight matrix is tasked with generating the input embedding for the task, while the output matrix will produce the predictions i.e. probability distribution over the entire vocabulary. This will make more sense in a bit. For now what is crucial to understand, is that each token has an embedding in the input word matrix. This will be its context embedding. Additionally each token has an embedding in the output word matrix which is its center embedding. In other words, each word has 2 embeddings. The tokens in the Vocabulary need to be mapped to indices, so we can look up their embeddings in the input or output matrices using indexing.\n\nFun Fact: Zyzzyva is an actual word and refers to a type of weevils. I asked Google: “What is the last item in English dictionaries.”\n\n\n\n\nLet’s put all we know together by illustrating the forward and backward pass! Below, we use the same example of center and context words.\n\nUsing the token-index mapping, we look up the indices of the center and context words.\nThe 4 context words are taken from the input matrix and stacked horizontally.\nTo get the average context, we take the row-wise average of the context word embeddings.\nUsing the output (center) matrix, we calculate the dot product between each word in the vocabulary and the averaged context embedding. This is a matrix-vector multiplication. In the resulting score vector each item refers to the similarity (dot product) between the average context and the words in the vocabulary.\nTo get a probability distribution over the vocabulary, we apply softmax normalization.\nThe y_true vector will be OHE where the single non-zero element is at the index of the center word.\nFinally, we apply Cross-Entropy loss and apply backprop to calculate the gradients.\n\n\n\n\n\n\nObjective Function Explained\nNow that we have a better understanding of the forward and backward pass of CBOW, let’s examine the math. Actually, the math of the “vanilla” Word2Vec is not too convoluted, we just need to dissect the objective function.\n\nQuick disclaimer: you won’t find these functions in the original paper. Here, I am making use of [2] and [3] but I make an effort at simplyfying the notation and provide a more detailed explanation.\n\nOur goal is to maximize the data likelihood i.e. to maximize the probability of a center word \\(w_{t-1}\\) conditioned on the context words in context window of c \\(w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c}\\), a given parameter set of \\(\\theta\\) which is the context and center embedding matrices. \\(T\\) denotes the tokens of the dataset i.e. all the words in the dataset.\n\\[\n\\text{Likelihood} = L(\\theta) =  \\prod_{t=1}^{T} P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c}; \\theta)\n\\]\nHow do we calculate \\(P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c})\\)? We are yet to inject the word embeddings into these functions because now they refer to words. In section “Illustrated Forward and Backward Pass”, we said that each word has a corresponding context (input) and center (output) word vector. Let’s denote the context word matrix as \\(K \\in \\mathbb{R}^{n \\times |V|}\\) where \\(k_{i}\\) refers to the i-th column of K meaning the context embedding of word at index i. The center word matrix is denoted by \\(U \\in \\mathbb{R}^{|V| \\times n}\\) where center word embeddings are stacked row-wise. Similarly, \\(u_{i}\\) refers to the i-th row of U indicating the center embedding of word at index i.\nLet’s now write up how to express \\(P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c})\\) using the contex and center word embeddings and then explain it in more detail. Below \\(u_{t}\\) refers to the context embedding of the context word at position t. \\(\\bar{k_{c}}\\) refers to the average context embedding of the context words. By taking the dot product between these two, we get a similarity metric (an unnormalized one) between the center word and the average context. The higher the dot-product the more similar the vectors are i.e. the vectors point to a similar direction.\n\n\n\nBy calculating the dot product between all center word embedding and the average context embedding in the vocabulary (the denominator) and applying softmax, we get a probability distribution over the entire vocabulary.\n\\[P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c}) = \\frac{exp(u_{t}\\bar{k_{c}})}{\\sum_{w=1}^{V}exp(u_{w}\\bar{k_{c}})}\\]\nIn machine learning, it is easier to work with the dual problem of maximizing likelihood which is minimizing the negative log likelihood. Using this trick makes the optimization easier. If you want to know why, I recommend reading this thread. Take the log of the product term results in the summation of logs. By convention, we optimize for the average log-likelihood, hence taking \\(\\frac{1}{T}\\).\n\\[\n\\begin{align*}\nJ(\\theta) & = -\\frac{1}{T} \\log L(\\theta) \\\\\n& = -\\frac{1}{T} \\sum_{t=1}^{T} \\log P(w_t | w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+c}; \\theta)\n\\end{align*}\n\\]\nOkay, so now we got a probability distribution over the entire vocabulary which is the prediction of the model but how do we relate this to the ground truth? The cost function does not (yet) include the y_true term though we use it for the cross-entropy loss calculation in the forward pass chart! I just left out this part from the cost function definition to not to make the formula too difficult to understand. The correct loss function for a given sample (a center word and its context) J is:\n\\[J = -\\sum_{w=1}^{V} y_{w}log(\\hat{y}_{w})\\]\nwhere\n\\[\\hat{y} = \\frac{exp(u_{t}\\bar{k_{c}})}{\\sum_{w=1}^{V}exp(u_{w}\\bar{k_{c}})}\\]\nSince the target, y vector is OHE, J reduces to where the i index refers to the target element of the target vector e.g. the index of “ChatGPT” as in the example:\n\\[J = -y_{i}log(\\hat{y_{i}})\\]\nWe can extend the formula to contain all training samples of the dataset but this is pretty much all about the CBOW version of Word2Vec. Except for one last thing. In the next section, we will review the training algorithm and point out its computation complexity and 2 methods that reduce the computational complexity: Negative Sampling and Hierarchical Softmax."
  },
  {
    "objectID": "posts/word2vec/index.html#skip-gram",
    "href": "posts/word2vec/index.html#skip-gram",
    "title": "Dachs2Num",
    "section": "Skip-Gram",
    "text": "Skip-Gram\nSkip-Gram flips the role of the context and center words. Here, the center word is used as input to predict the context (output) words. The forward and backward pass are illustrated below in a similar fashion as for CBOW. Observe the differences between the two architectures. Here, the input matrix is the one containing the center word embeddings, the output contains the context ones. We use the center word representation as input to calcute scores for the context outputs. Just like in CBOW, we make a softmax prediction over the entire vocabulary but we contrast these predictions against the multiple target context words which is indicated by the grey box on the right. In this example, we predict the 4 context words from the center.\n\n\n\nFor ease of notation, only a single sample’s training objective will be detailed. Let’s first start with the likelihood function again. Here, we aim to maximize the probability of context words conditioned on the center word. If we denote context lenght with c, there are 2c context words of which probabilities we aim to maximize (c history and c future context words).\n\\[\\text{Likelihood}=L(\\theta) =\\prod_{j=1}^{2c}P(w_{j} \\mid w_t; \\theta)\\]\nIt is important to note that the formula implies that we make a strong conditional independence assumption. Given a center word, all output words are completely independent (hence you can simply multiply the probabilities). This is definitely not true. Just think of simple grammar. If you have a noun center word followed by a verb context, then the upcoming context is quite unlikely to be a verb again. To give you an example, if you have the word Chelsea (a famous football club and also a district in London) as the center followed be played context then the next context is less likely to be a verb like jump and more likely to be an adjective like marvelously. Seeing this example you may even give semantical arguments why the conditional independence does not hold like nouns more related to football e.g. game is more likely to come up than academics related nouns e.g. conference. Nonetheless, Skip-Gram manages to work well in practice, thus we may be okay with this. By the way, it is the very same assumption that the Naive Bayes Classifier model makes. This naive assumption lets us to break out probabilities so we don’t have to deal with figuring out the joint distributions.\n[NOTE] I think an elaboration of this point may be useful\nThe softmax is used again to get the probability distribution over the vocabulary. Below \\(k_{t}\\) refers to the center word embedding while \\(u_{c}\\) to the context word embedding. This is the opposite of what we have seen in CBOW.\n\\[P(w_{j} \\mid w_t) = \\frac{\\exp(u_{j}k_{t})}{\\sum_{w=1}^{V}\\exp(u_{w}k_{t})}\\]\nIf you take the negative log-likelihood for a single context word, given a center:\n\\[\n\\begin{align*}\nJ & = -\\log P(w_{j} \\mid w_t) \\\\\n& = -\\log\\frac{\\exp(u_{j}k_{t})}{\\sum_{w=1}^{V}\\exp(u_{w}k_{t})} \\\\\n& = -\\left[\\log \\exp(u_{j}k_{t}) - \\log\\sum_{w=1}^{V}\\exp(u_{w}k_{t})\\right] \\\\\n& = -\\left[u_{j}k_{t} -  \\log\\sum_{w=1}^{V}\\exp(u_{w}k_{t})\\right]\n\\end{align*}\n\\]\nExpanding it to all context words:\n\\[ J = -\\sum_{j=1}^{2m}\\left[{u_{j}}{k_t} -log\\sum_{w=1}^{V}\\exp(u_{w}k_{t})\\right]\\]\nBy now, we have grokked the two versions of the Word2Vec algorithms. One detail is still missing though. As it was pointed out, each word has context and one center embedding. How to make one of them? Just simply average them! Next, let’s review some practical limitations and their remedies."
  },
  {
    "objectID": "posts/word2vec/index.html#expensive-softmax-and-alternatives",
    "href": "posts/word2vec/index.html#expensive-softmax-and-alternatives",
    "title": "Dachs2Num",
    "section": "Expensive Softmax and Alternatives",
    "text": "Expensive Softmax and Alternatives\nNote that in the softmax normalization, we normalize over the entire vocabulary which is huge. Its comoutational complexity is linear, \\(\\mathcal{O}(n)\\). In the case of Google News dataset it is ca. 6M tokens! Even if you have optimized, vectorized implementation of the softmax calculation, it would take very long to calculate the softmax for each sample in the dataset. As mentioned before, the 2 other innovations of Word2Vec, Hierarchical Softmax and Negative Sampling aim at this computational issue. To limit the scope of this blog post, we will only elaborate on Negative Sampling. While Negative sampling is a more straightforward concept and computationally more efficient, the results of Hierarchical Softmax are not better.\nAt this point I want to point out a little confusion on my part. Though in the first [1] paper, the authors introduced both Skip-Gram and CBOW, in the second they only proceeded with Skip-Gram without any explanation. At least I could not find any. The results in the first paper did not suggest any superiority of Skip-Gram. If you understand this decision, please let me know! :) Anyway, I will proceed in the same way as the authors did and use Skip-Gram accordingly.\n\nNegative Sampling\nThe idea of negative sampling, which was actually introduced in the second Word2Vec paper [2], is simple: instead of contrasting a given center and context word embedding pair against the entire vocabulary, why don’t we just contrast it against noise? In other words, what we aim here is to assess the probability that the center and context words came from the data. The trick is that we ingest negative (or fake if you like) context words so that we expect the model to tell these apart. If I have the center OpenAI and chatGPT as context, I want to assign high probability to this pair but low to a random pair like OpenAI and dachshund or OpenAI and daisy. We use k such negative samples for each positive one. The authors recommend 2-5 negative samples for bigger while 5-20 in smaller datasets. The likelihood function is given as below. \\(w_{t}\\) denotes the center, \\(w_{j}\\) the context word. The tilde marks the negative context sample.\n\\[\n\\begin{align*}\n\\text{Likelihood} & = P(D=1| w_{j}, w_{t}) \\prod_{i=1}^{K}P(D = 0| \\tilde{w_{j_i}}, w_{t}) \\\\\n& =  P(D=1) \\prod_{i=1}^{K}(1 - P(D = 1 | \\tilde{w_{j_i}}, w_{t}))\n\\end{align*}\n\\]\nWord2Vec uses the sigmoid function to model probability and for similarity we again use the dot product. Recall that \\(k_{t}\\) refers to the center word embedding while \\(u_{j}\\) to the context word embedding \\[\n\\begin{align*}\n\\text{Likelihood} & = \\frac{1}{1 + \\exp(-u_{j} k_{t})} \\prod_{i=1}^{K} \\left(1 - \\frac{1}{1 + \\exp(-u_{j_i} k_{t})}\\right) \\\\\n& =  \\frac{1}{1 + \\exp(-u_{j} k_{t})} \\prod_{i=1}^{K} \\frac{1}{1 + \\exp(u_{j_i} k_{t})}\n\\end{align*}\n\\]\nAgain, taking the negative log likelihood (NLL) as traning objective:\n\\[J = - log  \\frac{1}{1 + exp(-u_{j} k_{t})} - \\sum_{i=1}^{K} log \\frac{1}{1 + exp(u_{j_i} k_{t})}\\]\nIt seems that we have a loss function so we are all set! Almost… We still need to figure out how to draw negative samples i.e. define \\(P(\\tilde{w_{j_i}})\\). One might think of using a uniform distribution but that does not take into account word frequencies that we may want to take into consideration. The authors recommend using a scaled version of the unigram distribution \\(U(w)\\) (model based on word frequencies). They found empirically that the unigram distribution raised to the power of 3/4 \\(U(w)^{3/4}\\) works best. To build some intuition why this may be a particularly useful setting, let’s look at the below table of example. We have a 3 words, one is very frequent (at), another less frequent (cinema) and an infrequent one (dachshund). If we raise their relative frequencies to 3/4, then relatively, the infrequent dachshund gets 3x more frequent, cinema 1.7 while at only 1.5. You can see that by using this scaled version of unigram distribution for negative sampling, we increase the probability of drawing less frequent words on the expense of more frequent ones. This intuitively makes sense because without this, we may be drawing too similar negative samples e.g. “the, it” which does not seem to be noise, against we want to contrast the word embeddings.\n\n\n\n\n\n\n\n\nWord\nRelative frequency - \\(f(x)\\)\nRe-scaled relative frequency - \\(f(x)^{3/4}\\)\n\n\n\n\nat\n0.3\n0.45\n\n\ncinema\n0.1\n0.17\n\n\n…\n…\n…\n\n\ndachshund\n0.01\n0.03"
  },
  {
    "objectID": "posts/word2vec/index.html#subsampling-of-frequent-words",
    "href": "posts/word2vec/index.html#subsampling-of-frequent-words",
    "title": "Dachs2Num",
    "section": "Subsampling of frequent words",
    "text": "Subsampling of frequent words\nAnother trick that Word2Vec applies to put less emphasis on frequent words is subsampling. The authors argue that words like “in”, “the”, and “a” occur hundreds of millions of times in a large corpus of text. The co-occurence of these words arguably does not provide any useful information with other words e.g. “Paris”. However, the co-occurrence of other words e.g. “Paris” and “France” has immense useful information. Thus, it seems plausible to subsample the frequent words which speeds up training by skipping a share of the frequent word samples. Below, \\(P(x)\\) denotes the probability of x being skippend, \\(f(x)\\) the frequency of the word, \\(t\\) is a chosen threshold recommended to be around \\(10^{-5}\\). It is obvious that infrequent words will have very high probability of being picked. Frequent words are more likely to be skipped but this may be offset by a higher choice of \\(t\\) hyperparameter.\n\\[ P(x) = 1 -  \\sqrt{\\frac{t}{f(x)}}\\]"
  },
  {
    "objectID": "posts/word2vec/index.html#other-notes",
    "href": "posts/word2vec/index.html#other-notes",
    "title": "Dachs2Num",
    "section": "Other notes",
    "text": "Other notes\nThough this has been quite an in-depth Word2Vec explainer, there is still a lot more to explore within and beyond the scope of Word2vec. I am going to just share a few more things in an unstructured way. You may decide to explore any of these in more detail.\n\nHyperparameters\nThroughout the post, I tried to point out the recommended range of hyperparameters. [8] provides further comments and tips by the authors.\nRecommended HPs: - Embedding size: 50-300 - Context size: 5-10 - Algorithm: Skip-gram/ CBOW - #negative samples: 2-5 (if dataset big enough) if using Negative Sampling and not Hierarchical Softmax - Epochs: 1-3 - Subsampling threshold \\(t\\): ca. \\(10^{-5}\\)\nThe paper does not touch upon what optimizer to use but one might benefit from using an adaptive one like Adam. This might make sense because we may want to apply larger updates to less frequent words.\n\n\nDataset\nThe quality of the dataset is maybe more important than the specific HP settings we choose. The paper used the proprietary Google News dataset but there are now much bigger datasets! Just look at how GloVe [8] benefits from larger and better datasets.\n\n\n\n\n\nEvaluation\nEvaluating word vectors is an interesting task. There are 2 ways to do this: 1) intrinsic and 2) extrinsic. In intrinsic evaluation, typically, we do some kind of arithmetic of word embeddings and contrast it to our expectations. There are word similarity and analogy datasets that have been curated by humans. The authors open-sourced their own datasets for intrinsic evaluation: link. It contains semantic and syntactic analogies like “Athens-Greece+Madrid=Spain” and “occasional-occasionally+amazing=amazingly” In extrinsic evaluation, we see how the word embeddings influence the performance of downstream applications where these are used as inputs. Note, that the latter is less sterile because we not only expect “good” word embeddings but also a good interaction between them and the downstream-specific application algorithm.\n\n\nBias\nSince Word2Vec is trained on large corpus of data, it will be exposed of all the biases that surround us. Just think about how more often it will see male references to politicians or engineers and much less for kingergarten teacher or nurse. This begs loads of ethical questions and for solutions. This is a very important topic and I might devote more time to explore it more in a follow-up post. For now, you can take a look at [10] for more details or the “Bias and overgeneralization” part of [11]."
  },
  {
    "objectID": "posts/word2vec/index.html#acknowledgement",
    "href": "posts/word2vec/index.html#acknowledgement",
    "title": "Dachs2Num",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nI have made extensive use of the two Word2Vec explainers [1], [5] and the Stanford CS224N notes [3]. I have also used ChatGPT to correct my English and generally make the text more comprehensible."
  },
  {
    "objectID": "posts/summer-of-math/index.html#polysemanticity-and-capacity-in-neural-networks",
    "href": "posts/summer-of-math/index.html#polysemanticity-and-capacity-in-neural-networks",
    "title": "Integration by Parts for Archers",
    "section": "Polysemanticity and Capacity in Neural Networks",
    "text": "Polysemanticity and Capacity in Neural Networks\nLast but not least, here’s my own entry. It’s a walkthrough of the key concepts necessary to understand the paper “Polysemanticity and Capacity in Neural Networks”.\n\n\n\n\n\n\nJargon Alert\n\n\n\nThis is a video about ongoing research in AI Safety. You might not have the necessary background to understand everything.\nThat’s ok, I tried to introduce all relevant terms and abstract away the math with visual intuitions.\n\n\nFrom the papers abstract:\n\nIndividual neurons in neural networks often represent a mixture of unrelated features. This phenomenon, called polysemanticity, can make interpreting neural networks more difficult and so we aim to understand its causes. We propose doing so through the lens of feature capacity, which is the fractional dimension each feature consumes in the embedding space.\n\nIn the video, I explain:\n\nPolysemiticity and how it’s different from superposition\nThe impact of feature importance and sparsity on representation capacity allocation\nVisualizations of polysemantic and monosemantic neurons\nFeature capacity and its relation to fractional dimension\nBudget allocation plots and their analysis\nPhase diagrams for understanding feature allocation dynamics\n\n\nWhat’s your rating? I’d love to hear some feedback."
  },
  {
    "objectID": "posts/summer-of-math/index.html#conclusion-and-call-to-action",
    "href": "posts/summer-of-math/index.html#conclusion-and-call-to-action",
    "title": "Integration by Parts for Archers",
    "section": "Conclusion and Call to Action",
    "text": "Conclusion and Call to Action\nWhat a summer it’s been! From delving deep into mathematical wonders to appreciating the art of explaining complex topics, it’s been a whirlwind of learning and fun. And if there’s one thing I’ve taken away from this adventure, it’s the sheer joy of creating math explainers. So, dear reader, I urge you to dive into the world of math exposition. Craft your own stories. Share your passion. Who knows? Maybe next summer, I’ll be watching your video and getting my mind blown!"
  },
  {
    "objectID": "posts/#001 Rules for writing a Mini-Essay/index.html",
    "href": "posts/#001 Rules for writing a Mini-Essay/index.html",
    "title": "#001 Rules for writing a Mini-Essay",
    "section": "",
    "text": "Rules for writing a Mini-Essay:\nHave you ever felt overwhelmed by the tyranny of the blank page? Are you wishing to incorporate writing into your life but have failed to do so consistently?\nRecently, I started stacking another habit on top of my morning reading routine. Now, after reading, I take some time to reflect and write. My writing is guided by a few simple rules that help me stay focused. Here are the rules that help me produce 1 mini-essay per day:\n\nStick to 1 idea and 1 idea only\nKeep it short:\n\nTime limit: 10-20 minutes\nWord limit: 100-500 words\nPage limit: The essay should be visible on one page without the need to scroll\n\nKeep the essays organized\n\nUse a Kanban board to track the process through all stages from raw idea to published\nInclude Tags and Folders for easy retrieval \nLink to other ideas that are relevant (Maybe your own past posts)\nReferences (Where did you get this idea? What prompted it? Books, videos, articles)\n\nThe three part structure:\n\nIntro\n\nTry any of the following: Introduce curiosity with a Hook / Be clear about the Premise / Use a bold statement / ask a provoking question / evoke strong emotions / throw the reader straight into the action by means of a story\n\nBody\n\nAnswer the premise of the intro\nOptional: Try a Story Note (use story telling to bring a point across)\n\nChoose the point you’re trying to make\nAmplify the message by choosing an illustrative story\nFor hard topics depersonalize the story by using non human characters or humor\n\n\nConclusion\n\nKeep it short and simple (KISS) - 1 or 2 sentences\n\n\nFeedback (3 sanity checks)\n\nRead the essay out loud to assess how it flows. This is an unreasonably powerful tool to quickly notice and change odd sentences\nAsk an AI to rate the essay & provide feedback on how to improve it\n\nHere’s my custom GPT - Mini Essay Critic\n\nAsk another person for feedback and written comments\nOptional: Wait a few days to let the idea simmer before returning to it with fresh eyes and doing a few last changes.\n\nOptional: Include an illustration\n\nMake a custom graphic for more important posts (with Excalidraw, Figma or Miro) or\nUse an existing graphic if\n\nPreserving precision of information is important (e.g. data plots) or\nCreating a custom graphic would be prohibitively time intensive\n\n\n\nFollowing these simple rules can transform your writing routine into a structured and productive daily habit.\n\n\nReferences\nhttps://www.youtube.com/watch?v=N4YjXJVzoZY&ab_channel=Odysseas\nhttps://www.youtube.com/watch?v=eCaOSNxwCsw&ab_channel=Odysseas"
  },
  {
    "objectID": "posts/covid19-boredom-and-artificial-intelligence/index.html",
    "href": "posts/covid19-boredom-and-artificial-intelligence/index.html",
    "title": "COVID-19, boredom and artificial intelligence",
    "section": "",
    "text": "Smelly Vagabounds(,) and Thinkers\n\nI highly recommended listening Epic Mountain while reading this piece.\n\nIt all started last year when I began reading “The master algorithm” by Pedro Domingos. The smell of sweaty hungover travelers was thickening the air in the Columbian 10-bed dorm I found shelter in. Although I would later find out how little I actually understood, a profound feeling of understanding the complex world we have to navigate, creeped up my sleeve as I kept on reading about artificial intelligence, algorithms, and philosophy. Wait a moment - philosophy? Yeah, philosophy. Many brilliant thoughts that Domingos writes about started out their journey in the head of a thinker’s curiosity-driven pursuit of knowledge - ergo philosophy. As you might guess the epiphanic feeling of understanding “what the heck was going on in the world of tech” didn’t last long. Soon enough I was not only surrounded by smelly vagabonds but also by heavily complex thoughts ranging from philosophy to computer science and back to the world of mathematics. A little lost and confused I decided to put down the book and get to grips with the basics first. That’s when I decided to devote the majority of my time reading and studying philosophy. The next image is a depiction of me. Literally, go ask my friends!\n\n\n\n\n\nFast-forward, December 2019. Over the last couple of months, I devoted a good chunk of my time pondering philosophical questions, when I got curious again. Now that I had at least a basic understanding of philosophy, I remembered that there was something else that bugged my mind. Algorithms, Programming and Mathematics. I should start getting technical right? - Yep. And so I did. Well, at least I tried. Getting into programming and refreshing some of my mathematical knowledge wasn’t as easy as previously thought.\nWeeks later, near the end of February 2020, I finally managed to pull through the strenuous task of finishing my first online Machine Learning Course. Did I enjoy it? Well… It was tough, and I was nowhere near to programming my first line in Python, which by now I knew was the go-to programming language for Machine Learning. But then again, did I even try learning to write any Python code? Mhm, let’s skip that part. Being the problem solver that I am I tried to find a better, more satisfying way to get into AI. I was determined to learn. Hell, so determined that I walked around my new hometown Madrid for 3 weeks on end to talk to people who’d already been navigating the complex world of machine learning, data science and artificial intelligence. After several failed attempts to find a suitable environment to learn, I got lucky. The Immune Coding Institute offered a Data Science Bootcamp, and I was accepted to partake! I jumped on that chance and grabbed it by the balls.\n“Finally, every day I’ll be surrounded by like-minded people with whom I’ll share a common goal: Becoming a Data Scientist.” - At least that’s what I thought. Invigorated, enthusiastic and full of joy I completed the first two sessions of the Bootcamp only to wake up the next day to realize that the Corona Crisis had reached Madrid. Another setback to my efforts of becoming a Data Scientist? No, not this time!\n\n\nRegnosis: The Corona Crisis, a blessing in disguise?\nLike many of you out there, I am trapped at home, prohibited from going out. It’s shit, right? Well, actually, no! Why’s that, you might ask. Here’s the thing, let’s make a thought experiment, good ol’ philosopher-style. In hindsight, what will you think of today’s situation 6 months from now? Go ahead, take a couple of minutes, maybe write down your thoughts, it’s a great way to stay sane!\n\n\n\n\n\nHere’s what I think. 6 months from now many of us will look back at today and revisit the boredom that they experienced and how they went about changing their relationship to it:\n“Wow, I never thought that the Crisis was such a blessing. All the time it freed up for me. I was so stressed every day, running from one obligation to another, depriving myself of spending time with my friends and family. But then, when I was forced to stay home, it kind of clicked. First, I was bored and tended to binge on Netflix, but that got boring quite quickly too. The realization came late, but when I realized that this crisis was a reminder of all that’s important in life, I quickly gained control over the situation. I remember the day that I finally found time to do what I was longing to do for so long. I eventually started”XYZ” (writing/ playing music/ learning a new language) - fill in whatever dream of yours you wanted to pursue but lacked the time to do. Damn, if it wasn’t for the crisis I might still be wondering what it was like to do “XYZ”.\nPeople who realize this fast enough will be able to act upon the newly freed time and will profit tremendously in whatever area they choose to apply it to.\n\n\nCall to arms\nFor those of you who have not already changed the Browser Tab to start another binging marathon on Netflix, firstly, thanks. Secondly, here’s the deal! I want you to take 5 minutes and think of everything that you always wanted to do but never “found time for”. Then pick one thing and start acting on it.\nAs I know that it’s always easier said than done, I encourage you to follow my lead. I’ll have a weekly posting schedule where I’ll blog about my journey of becoming a Data Scientist. I want you to engage with me and comment on what you did in your personal project.\nOk, that’s basically it. If you’re done, feel free to write a comment down below or hit me up on LinkedIn to help each other stay motivated on our journey.\n\n\nGratefulness:\nBefore ending this post I would like to give a big thank you to the following people for inspiring me to take action and embark on a crazy journey to becoming a Data Scientist:\n\nAndrei Neagoie and Daniel Bourke for being kick-ass instructors that know how to ignite their students’ passion for a subject as complex as machine learning. Check out their Course if you’re interested in Data Science\nImmune Coding Institute for believing in me and giving me a chance to study Data Science at their wonderful facilities\nPedro Domingos who wrote the challenging book without whom I wouldn’t have had the curiosity to dive deeper and gather the necessary knowledge to understand his brilliant work\nMy girlfriend for preparing world-class scrambled eggs to free up time for my projects\nMy family for supporting me mentally and financially on my crazy adventures\nEpic Mountain for making inspiring music that transforms your brain into a pondering - machine!"
  },
  {
    "objectID": "posts/to-be-or-not-to-be/index.html",
    "href": "posts/to-be-or-not-to-be/index.html",
    "title": "To be or not to be",
    "section": "",
    "text": "To be or not to be\n\nWhat I’ve learned from trying too hard.\n\nIn what follows I’ll explain how to start a new endeavor and stick with it. More concretely the notion of identity-based and goal-based achievement will be discussed in consideration of how to develop the necessary mindset that results in sustainable, repeatable and pleasurable habits which in turn will yield a fruitful ground for achievement.\nA few weeks back an excited youngster sat at his desk trying to make sense of a bad situation and assign it some meaning. He would go on to formulate his ideas in a rush of feel-good endorphins. The pounding of keys would finally end after a short distance writing-sprint. After adding a witty title here and some funny pictures there, his first written piece was ready to be published. That should do for a first blog post, right? Right. Click - post - share - ask for feedback - get lavish praise by friends - wait until next week - hope to be blessed with random sparks of creativity - write again - repeat - become a writer - mission accomplished! As if, haha! Now who am I talking about, you’re asking? - Drumroll - That someone was me. Now some of you might ask what’s your point here? Isn’t that exactly how it works? Well no, here’s why.\n\n\nGoal-Based - Efforts\nThe above-described situation perfectly demonstrates how goal-based efforts work. As I set out on my journey the only thing on my mind was the goal: “Write a blog”. Now, a goal in and of itself is not a bad thing, but neither is it good, a goal is just a goal.\n\n\n\n\n\nGoal-Based and Identity-Based EffortsAs seen in the diagram above, goal-based efforts work from the outside in. The typical goal-setting process starts out by defining what it is that you want to achieve. Let’s say your goal is to lose weight. You begin to produce certain outcomes, like losing a pound here and there due to training and eating right. The actions you take might or might not lead to habits, that eventually become automated as you go to the gym every Monday, Wednesday, and Friday. Finally, if you’re in luck, you’ll finally incorporate being sporty, as part of your life. This process might take weeks, months or even years in some cases, but let’s be honest, we’ve all tried and failed at something like this, which leads makes me think, that there has to be a better way. Back to the story.\nIn my last blog post I proudly claimed, that I would upload 1 post per week in order to capture my journey of becoming a Data Scientist and Writer. Well, did you see me write anything in 3 weeks? Exactly, neither did I. Why? Mostly, because the anticipated exhilarating bursts of creativity didn’t appear according to plan, which quickly let me question my ability to write. This is by no means a valid excuse, which is precisely why I’m all the more thankful for what followed!\nA little beaten down by reality, an old friend of mine, Tom, taught me a valuable lesson. He asked me the following: Are you a writer or do you want to become one? In essence: “to be or not to be?”\n\n\n\n\n\nFor those who want to understand the profound contemplation behind this quote, and not just my misuse of it, check out this fun Youtube Video!\nTom went on to explain the problem with goal-based efforts. Setting goals, followed up by painstakingly following down that road, without ever stopping and asking “Why?”, is the equivalent of going on a hike without any shoes. In short, the journey will be very painful, unenjoyable and over, rather quickly. You’ll give up on your hike before it even began.\n\n\nIdentity-based efforts:\nAnother approach that works much better than goal-based efforts, is to work from the inside out and focus on your identity first, Tom said. A bit confused I asked: “what do you mean by that?” Patiently, he elaborated: Before starting out on a new journey, ask yourself these fundamentals:\n\nWhy do I want to do this? What’s the purpose of all this?\nDo I see myself doing this, 10 years from now?\nIs this something I enjoy or can come to enjoy?\nAm I willing to invest at least a couple of minutes, every day?\n\nIf the answer to most of those is “yes!”, or “I don’t know, but I’m sure as hell willing to find out”, congratulations! You’re now officially a “(fill in whatever it is that you want to be)”. You’re probably still a bloody amateur, but that’s alright.\nI pondered these questions in the back of my mind, as Tom continued teaching. “You have now started to grow. You’ve incorporated a certain topic as an integral part of your life! This is a great leap forward because now, you don’t have the pressure to reach a certain goal anymore. Shifting your perspective away from goals will allow you to view every step onwards as an opportunity to learn. You’ll be able to improve who you already are, rather than trying to become someone. This way you recognize your own imperfections and take the actions needed to even them out. Soon enough, this newfound identity you’re shaping will result in habits that positively reinforce your growth. Your efforts will materialize in outcomes and you’ll lose weight, write something or do whatever it is that you wanted to do in the first place. In summary, you’ll set yourself up for success. You might not achieve one or another thing in the short run, but you’ll never quit on your new endeavor because now it’s part of who you are!” Woah. Let that sink for a moment. It took me 2 weeks to internalize this!\n\n\nConclusion:\nSo, what’s the main takeaway and how does it all tie back to my efforts of writing blogs? What Tom taught me is to “be”, rather than “become”. The hardest part about setting out on a new journey is starting and setting yourself up for success. By accepting that you are already a writer, a runner etc. you’ll eliminate the pressure of becoming that someone. This, in turn, allows you to shift your perspective towards improving yourself. In short, don’t try hard, try smart. Remember, the following:\n\nGoal-based efforts can only get you so far\nIdentity-based efforts are a surefire way to succeed in the long run\nAsk “Why?” before you start\nBuild habits and results will follow\n\n\n\nGratefulness:\n\nThank you, Tom Munk, for opening my eyes to this fundamental truth about any effort whatsoever in life. I’m back on track and I don’t plan on derailing. I owe you a lot and I’m planning on repaying it fully!\nThank you, Andrei Neagoie, for teaching me about the truths about learning, habits and compound learning."
  },
  {
    "objectID": "ideas.html",
    "href": "ideas.html",
    "title": "Unashamed Curiosity",
    "section": "",
    "text": "VINC explainer\nELK Github repo Readme Visulaization\nDebate GPT-4 on when humans will cease to matter\nThe master salve dialectic in relationship to GPT4 “The basic function of a master is not to serve as a model of rational reasoning, providing the ultimate argument for adopting a certain position, but on the contrary, to surprise us by uttering statements which run against our hitherto doxa”\nKeep a research log for notodai What did we learn this week, how are we going to change our research? What are the open questions?\nHave GPT4 help me figure out what success means to me\nWrite a letter to yourself in 5 years\nWrite about meaning and work in the age of AGI\nRework this post for the website https://hackmd.io/ErH1A0HORmu59kgHVOwlJw\nRevive Cheri chronicles and post them\nPost my monthly planning and introspection routine and invite feedback\nDo some fun fermi estimates alone and compare with GPT-4\nAsk GPT to do cognitive task analysis. and naturalistic decision making\nLearn Prompt Engineering https://www.promptingguide.ai/ https://learnprompting.org/\nWrite about the anthropic principle https://en.wikipedia.org/wiki/Anthropic_principle\nA (nihilistic) crisis caused by rapidly accelerated AI capabilities/ Coud also be a post about how to deal with life: optimistic nihilism (EEAO and Kurzgesagt)\nWrite about strong inference https://en.wikipedia.org/wiki/Strong_inference#:~:text=In%20philosophy%20of%20science%2C%20strong,at%20the%20University%20of%20Chicago.\n\nhttps://www2.clarku.edu/faculty/pbergmann/SCEP/Platt%201964.pdf 18. Post this blog in SEASONS. Write about the benefits of short burst of intense writing interspersed with longer breaks to reflect, read, write and share/disseminate ideas. 19. Write up governance consultancy idea 20. Write about giving advice followed by anti advice 21. What is backchaining and how can I use it for my orgs strategy? 22. Explain Bisection algorithm (brilliant: https://brilliant.org/courses/calculus-done-right/computing-limits/ivt-application-roots/1/) with Polya conversation method (How to solve it) 23. Research how fat are the tails of research? 24. You admire the man who pushes his way to the top in any walk of life while we admire the man who abandons his ego. 25. Write a post titled: “I’m a high ego individual. And I’m proud(ish) 26. Comparative advantage: I think I’m at a juncture.\nI’m potentially making a mistake of I proceed in trying to become a researcher myself.\nThe DIY approach is useful to be able to recognise good work. I think I’m at a point where I can recognizer good research. (I should test this assumption and read a few papers that I should rate) I can also roughly do this for engineers and science communicators.\nWhat are the other things I need to become skilled at to identify skilled workers to hire?\nWhat do I want to achieve in the first place thst I need to hire someone for?\nI think I should exercise my ability to hire someone to do a task for me. Rommel is a good example for how to much DIY can be debilitating and unproductive.\n\nPut together a strategy for how to create visible output: “It’s now critically important to get into the real world and really challenge yourself. Expose yourself to risk Put yourself in situations where you will succeed or fail by your own decisions and actions, and where that success or failure will be highly visible.” - Marc Andreesen\nWrite about Positive disintegration https://en.wikipedia.org/wiki/Positive_disintegration\nBuild an automated weekly checkup tool\n\neveryone can fill it out on their own it creates a report and communicates it to the key decision-makers it creates automatized individual messages and sends them to the team members if there’s personal feedback for them in the form.\nThis could be a slack bot\n\nDigital Dream Map & Idea Generation for an exciting life\nCreate ARENA local chapter krakow\nOn Toughness - https://www.youtube.com/watch?v=PyosFtMazPo&ab_channel=ProductivityGame\nWrite about why I find AI development confusing (Inspired by Jack Clark)\nWrite about AI and Cyber Immune Systems (Inspired by Jack Clack and https://arxiv.org/abs/2308.05978). Read Immune book by Kurzgesagt\nWrite what I think about risk taking. Calculated vs stupid risk.\nWrite about embracing short term plans\nHow much money could we rase for EAG with sport events (i.e. EA Charity Marathon)\nMatch Quality - The Dark Horse Project\nGive a cuddle cave speech:short bits of wisdom and people discuss\nI’mm feeling a shift. I should act instead of spending most of my time learning.\nProduct: llm live translation pin\nWhy not try and do something stupendously large?\nWrite about connecting knowledge from disparate field i.e. arrowsmith software https://en.wikipedia.org/wiki/Arrowsmith_System -undiscovered public knowledge\nIdea : Create platform like innocentive for ai safety (https://www.wazokucrowd.com/story-of-innocentive/#:~:text=Purchase%20by%20Wazoku,unique%20innovation%20at%20scale%20offering. )\nVideo series : ai safety concepts in 1minute (Contact Karl von Wendt)\nUpgradable James Norriss (Contact the guy, he’s an EA)\nWrite about match quality (Malamuth - Scott’s vs Wales and Brits, Treat career like dating , Quitters are winners - Angela Duckworth, Grit = passion + perseverance, Miller - match quality = multi armed bandit problem , Shirt term &gt; long term planning , Peter Drucker , The dark horse project\nMichel - the context principle , Gilbert predictors and reflectors changing values\nFirst act then think - Discovery of self by doing stuff\nTreating symptoms vs causes\nHow to become more ambitious? what is ambition in the first place?\nStanford Study on walking an creativity (From Range book)\nHave a socratic dialogue about risk (with Chat GPT)\nCan I invent sustainable development goals for AI?\nLearn to do something yourself, then automate it (human or computer) by teaching someone to perform the task well or finding someone who’s much better at it than you (which you’d like to be able to evaluate)\nNanospecializations - You’re specialised for the task at hand. You can be a guitarist but now you’re playing a particular song. You’ll forget it in the future if you don’t train. E.g. Song: “Final countdown”\nWie funktioniert ein Kompass und wie kann ich meinen Tunen?\nI feel like I’m on a main Quest to build accurate world models (This is what motivates becoming a great generalist or many model thinker or polymath) and why I think it’s important to refuse to choose\nI value good rhetoric. I should learn more about it.\nWould you describe yourself as hardcore?\nCopy Holden Karnofsky really hard. Analyze his writing style and emulate it until you’re awesome at it. (Transcend your masters by shameless copying)\nWhen would it ever be warranted to have a private jet?\nhow to love someone average if you have an allergy against average\nThe thing with brutal honesty\nEncoding Strategies and the Generation Effect https://www.youtube.com/watch?v=JDb7EqCtlQc&ab_channel=BenjaminKeep%2CPhD%2CJD\nRegret. Would I regret it if i didn’t go to SF?\nWhat is a thing that is much larger than myself that I can work towards?\nMusic, Open Jam sessions and Hit Snap Clap like things one can do with a crowd.\nExplore my weird relationship with information density\nTo marry or not to marry"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "002 Babies are Like Tripping Adults\n\n\n\n\n\n\n\nMini-Essay\n\n\n\n\nChildren, with their fresh perspectives and less pronounced sense of self, possess a heightened ability to perceive reality similar to adults under the influence of psychedelics.\n\n\n\n\n\n\nMay 22, 2024\n\n\nKay Kozaronek\n\n\n\n\n\n\n  \n\n\n\n\n001 Rules for writing a Mini-Essay\n\n\n\n\n\n\n\nWriting\n\n\nMini-Essay\n\n\n\n\nA few simple rules for establishing a structured and productive daily writing habit.\n\n\n\n\n\n\nMay 21, 2024\n\n\nKay Kozaronek\n\n\n\n\n\n\n  \n\n\n\n\nIntegration by Parts for Archers\n\n\n\n\n\n\n\nIntegration by Parts\n\n\nCalculus\n\n\nMathematics\n\n\n\n\nWanna become a master at Integration by Parts? Let me tell you about my experience with The Summer of Math Exposition by 3Blue1Brown\n\n\n\n\n\n\nAug 22, 2023\n\n\nKay Kozaronek\n\n\n\n\n\n\n  \n\n\n\n\nDachs2Num\n\n\n\n\n\n\n\nML\n\n\nAI\n\n\nNLP\n\n\nWord2Vec\n\n\n\n\nDiscover how to make computers understand the awesomeness of dachshunds. An explainer of Word2Vec.\n\n\n\n\n\n\nApr 29, 2023\n\n\nGerold Csendes, Kay Kozaronek\n\n\n\n\n\n\n  \n\n\n\n\nTo be or not to be\n\n\n\n\n\n\n\nGoals\n\n\nIdentity\n\n\n\n\nWhat I learned from trying too hard: How to start and stick to a new endeavor by shifting your perspective from goal-based to identity-based efforts.\n\n\n\n\n\n\nApr 15, 2020\n\n\nKay Kozaronek\n\n\n\n\n\n\n  \n\n\n\n\nCOVID-19, boredom and artificial intelligence\n\n\n\n\n\n\n\nCovid\n\n\nAI\n\n\nPhilosophy\n\n\n\n\nDiscovering the hidden blessing of the pandemic, finding time for what matters and embarking on a journey to becoming a data scientist.\n\n\n\n\n\n\nMar 25, 2020\n\n\nKay Kozaronek\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "What I’m Doing Now",
    "section": "",
    "text": "This is a now page, inspired by Derek Sivers and Thomas Frank.\nI treat this as a public declaration of my priorities and a reminder to myself. I am open to considering new opportunities, as I highly value novelty and growth. However, I aspire to stick to my priorities 70% of the time. As a result, I will say no to the majority of requests that come my way now.\nIf my activities or priorities happen to change, I’ll update this page to reflect those changes.\nUpdated August 21th, 2023"
  },
  {
    "objectID": "now.html#setting-up-a-stable-homebase",
    "href": "now.html#setting-up-a-stable-homebase",
    "title": "What I’m Doing Now",
    "section": "Setting up a stable homebase",
    "text": "Setting up a stable homebase\n\nMy partner and I have been traveling a lot in the past years. We realized that we need more stability.\nNow we’re spending a few low-key, habit-driven weeks in the polish countryside.\nAfter a short travel stint I’ll be establishing a more stable living situation (Likely in Krakow or London)"
  },
  {
    "objectID": "now.html#speaking-at-effective-alltruism-global-eag",
    "href": "now.html#speaking-at-effective-alltruism-global-eag",
    "title": "What I’m Doing Now",
    "section": "Speaking at Effective Alltruism Global (EAG)",
    "text": "Speaking at Effective Alltruism Global (EAG)\n\nI will be speaking at EAGx in Berlin\nTitle: Level Up: Leveraging Self-awareness, Effective Learning, and Community\nDescription: In this interactive workshop, Kay delves into the art of continuous development, blending effective learning strategies with emotional intelligence and community support. Participants will explore self-awareness techniques, navigate the emotional challenges of learning, and harness the power of community. Ideal for individuals at any career stage seeking to enhance their learning journey, attendees will leave with actionable strategies, from “Habit Contracts” and “Nonviolent Communication” to assembling “Feedback Fellows”, ensuring a holistic approach to personal and professional growth."
  },
  {
    "objectID": "now.html#personal-development-and-exploration",
    "href": "now.html#personal-development-and-exploration",
    "title": "What I’m Doing Now",
    "section": "Personal Development and Exploration",
    "text": "Personal Development and Exploration\n\nI’m absorbed in thinking about my values and identity. I’m currently writing a post about a valuable exercise I did.\nA related recurring question is whether I’d like to be a generalist or specialist. Right now I am the former. Do I want it to stay this way? How can I lean in a lot more into this and how can I become a better generalist?\nTo answer these questions I’m experimenting with coaching to help me become more excellent.\n\nThis is also related to trying to find tactics for how to get more real life feedback. I’m building this website precicely for that purpose. Occasionally, the idea of revamping my personal YouTube channel comes up.\nAn idea that might be helpful in making progress more rapidly is that of forming useful not true beliefs.\nI’m exploring how I’d like to continue my path and what kind of work I’d like to produce:\n\nI’d like to become a better leader\nI’m spending some time on exercises for how to be a better strategic thinker\n\n\nThese thoughts have been inspired by the following quote:\n\n“It’s now critically important to get into the real world and really challenge yourself. Expose yourself to risk Put yourself in situations where you will succeed or fail by your own decisions and actions, and where that success or failure will be highly visible.” - Mark Andreesen"
  },
  {
    "objectID": "now.html#ai-safety-entrepreneurship",
    "href": "now.html#ai-safety-entrepreneurship",
    "title": "What I’m Doing Now",
    "section": "AI Safety & Entrepreneurship",
    "text": "AI Safety & Entrepreneurship\n\nI want to transition away from research to more managerial tasks. My next steps in AI Safety will likely involve doing more strategic and organizational work. I’m exploring ideas for companies to start, who to do it with and where to do it. Until then I am finishing several other projects.\nI have submitted a video about Polysemanticity and Capacity in Neural Networks to the Summer of Math Exposition by 3Blue1Brown. As part of the competition I had to review other participants entires. Find out more in my post titled “Integration by Parts for Archers”\nI’m currently working on a video about Causal Scrubbing. For this project I’m experimenting with delegating tasks via upwork to become better at managing projects and people. It’s very fun.\nI’m slowly reviewing the Mechanistic Interpretability content from the Alignment Research Engineering Accelerator (ARENA) curriculum.\nIn October I might be spending some time in Vermont to do an AI Safety related project."
  },
  {
    "objectID": "now.html#learning-and-education",
    "href": "now.html#learning-and-education",
    "title": "What I’m Doing Now",
    "section": "Learning and Education",
    "text": "Learning and Education\n\nI’m reading a lot:\n\nAtomic Habits\nRefuse to Choose!\nRange\nThe Personal MBA\nExcellent Advice for Living\nSPRINT\n\nDoing daily math at Brilliant.org:\n\nLinear Algebra with Applications\nProbability Theory\nGeometry\n\nI’m learning more about mental-wellbeing:\n\nTo become better at noticing things and communicating my needs, feelings and requests I use the Nonviolent Communication framework (NVC)\nAn interesting therapeutic approach I have been introduced to is Positive Disintegration"
  },
  {
    "objectID": "now.html#health-and-exercise",
    "href": "now.html#health-and-exercise",
    "title": "What I’m Doing Now",
    "section": "Health and Exercise",
    "text": "Health and Exercise\n\nAs an early birthday gift to myself I am planning to run a Marathon on the 21st of April 2024\nTo that end I am preparing to participate in the 9th Cracovia Royal Half Marathon on October 8th, 2023. This includes daily runs and morning stretches as well as some full body HIIT units.\nI will take 1 week of surfing lessons this year\nEating healthily again\nMy sleeping schedule is strict and I prioritize getting 8-9 hours of quality sleep."
  },
  {
    "objectID": "now.html#miscellaneous-side-projects",
    "href": "now.html#miscellaneous-side-projects",
    "title": "What I’m Doing Now",
    "section": "Miscellaneous Side Projects",
    "text": "Miscellaneous Side Projects\n\nI’d like to explore some prompt engineering techniques to become better at interacting with AI and generate ideas for model evaluations:\n\nLearn Prompting\nPrompt Engineering Course on Coursera\n\nPracticing the guitar every day\nI’m thinking about doing the MITx MicroMasters Program in Data Economics and Design of Policy (DEDP) with the eventual goal of studying at MIT.\n\nThis would result in making heavy use of Anki again.\n\nFun and humor are very important to me. To enrich my daily life and improve my public speaking skills I’ve signed up for the course Stand Up!; Comedy Writing and Performance Poetry. The goal is to produce a 5 minute standup comedy routine that I can perform sometime in November at a biweekly Open Mic (English) in Krakow.\nRecently I have received lots of coaching and I realized that I have the necessary skills to try coaching others myself."
  },
  {
    "objectID": "collaborators.html",
    "href": "collaborators.html",
    "title": "Collaborators",
    "section": "",
    "text": "This is a blog about AI safety, cognitive science, philosophy, and much more. The goal is to explore whatever we’re interested in. To that end I (Kay) am collaborating with a collective of unashamed, curiosity-driven explorers who love quirky art and dachshunds. Here are the wonderful collaborators of this blog.\nAre you ready to open Pandora’s box?"
  },
  {
    "objectID": "collaborators.html#gerold-csendes",
    "href": "collaborators.html#gerold-csendes",
    "title": "Collaborators",
    "section": "Gerold Csendes",
    "text": "Gerold Csendes\n\n\n\nGerold’s Image\n\n\nWith a background in business mathematics, applied economics, and data science, Gerold is no stranger to the world of AI. Currently, he’s applying his skills as an AI Engineer at a cancer research startup, which is just as intense as it sounds. But don’t let the serious job title fool you. Outside of work, Gerold is a dedicated dancer who’s convinced that he’s the protagonist in a scene straight out of “The Scent of a Woman.” He’s also the proud parent of two dogs and an unabashed victim of Kay’s nerd sniping.\nDespite living far from Italy, Gerold has a romantic notion of the country that he believes adds a dash of charm to his personality. If he’s not crunching numbers or practicing his tango, you might find him training for ultra-long-distance hikes. He recently finished a 56 km hike in a single day and he’s aiming for the 100 km mark. Who says AI engineers can’t be outdoorsy?"
  },
  {
    "objectID": "collaborators.html#kay-kozaronek",
    "href": "collaborators.html#kay-kozaronek",
    "title": "Collaborators",
    "section": "Kay Kozaronek",
    "text": "Kay Kozaronek\n\n\n\nKay’s Image\n\n\nIn the world of machine learning, Kay is a research engineer who’s on a mission to ensure advanced AI systems are as safe as a cushioned playground. With a background in management consulting and startup experience, Kay sees the creation of these safe AI systems as the most pivotal challenge of our time. His research focuses on the interpretability of large language models, a task as complex as figuring out what his partner wants for dinner.\nKay values clear communication, honest feedback, and a good dad joke. If you ever need a pun to lighten the mood, he’s your guy. When he’s not deciphering AI or cracking jokes, you’ll likely find him immersed in sports, creating music on his piano or guitar, or simply spreading smiles.\nIf he hadn’t taken up the mantle of AI safety, Kay would probably be a jazz cat, hopping from city to city, playing soulful tunes in smoky jazz clubs with strangers. Fortunately for us, his love for AI is as strong as his love for music.\nKay is particularly proud of his 3-month solo trek through South America, during which he visited 7 countries and picked up Spanish, all without any prior knowledge. The adventure is captured in a series of vlogs on YouTube, showcasing Kay’s quirky documentation style. He’s also incredibly proud of his transition from business administration student to professional AI Safety researcher, thanks to nearly two years of self-study and a grant from OpenPhilanthropy. Talk about a plot twist!"
  },
  {
    "objectID": "posts/001-rules-for-writing-a-mini-essay/index.html",
    "href": "posts/001-rules-for-writing-a-mini-essay/index.html",
    "title": "001 Rules for writing a Mini-Essay",
    "section": "",
    "text": "Rules for writing a Mini-Essay:\nHave you ever felt overwhelmed by the tyranny of the blank page? Are you wishing to incorporate writing into your life but have failed to do so consistently?\nRecently, I started stacking another habit on top of my morning reading routine. Now, after reading, I take some time to reflect and write. My writing is guided by a few simple rules that help me stay focused. Here are the rules that help me produce 1 mini-essay per day:\n\nStick to 1 idea and 1 idea only\nKeep it short:\n\nTime limit: 10-20 minutes\nWord limit: 100-500 words\nPage limit: The essay should be visible on one page without the need to scroll\n\nKeep the essays organized\n\nUse a Kanban board to track the process through all stages from raw idea to published\nInclude Tags and Folders for easy retrieval\nLink to other ideas that are relevant (Maybe your own past posts)\nReferences (Where did you get this idea? What prompted it? Books, videos, articles)\n\nThe three part structure:\n\nIntro\n\nTry any of the following: Introduce curiosity with a Hook / Be clear about the Premise / Use a bold statement / ask a provoking question / evoke strong emotions / throw the reader straight into the action by means of a story\n\nBody\n\nAnswer the premise of the intro\nOptional: Try a Story Note (use story telling to bring a point across)\n\nChoose the point you’re trying to make\nAmplify the message by choosing an illustrative story\nFor hard topics depersonalize the story by using non human characters or humor\n\n\nConclusion\n\nKeep it short and simple (KISS) - 1 or 2 sentences\n\n\nFeedback (3 sanity checks)\n\nRead the essay out loud to assess how it flows. This is an unreasonably powerful tool to quickly notice and change odd sentences\nAsk an AI to rate the essay & provide feedback on how to improve it\n\nHere’s my custom GPT - Mini Essay Critic\n\nAsk another person for feedback and written comments\nOptional: Wait a few days to let the idea simmer before returning to it with fresh eyes and doing a few last changes.\n\nOptional: Include an illustration\n\nMake a custom graphic for more important posts (with Excalidraw, Figma or Miro) or\nUse an existing graphic if\n\nPreserving precision of information is important (e.g. data plots) or\nCreating a custom graphic would be prohibitively time intensive\n\n\n\nFollowing these simple rules can transform your writing routine into a structured and productive daily habit.\n\n\nReferences"
  },
  {
    "objectID": "posts/002-babies-are-like-tripping-adults/index.html",
    "href": "posts/002-babies-are-like-tripping-adults/index.html",
    "title": "002 Babies are Like Tripping Adults",
    "section": "",
    "text": "Babies are Like Tripping Adults\nImagine experiencing every sight, sound, and sensation as if for the first time. Are children, with their fresh perspectives, untainted by experience, more conscious than adults? This essay explores the intriguing similarity between children’s heightened perceptions and those reported by adults under the influence of psychedelics.\nTo begin exploring this idea, we’ll define consciousness as having a sense of self coupled with the ability to perceive. Let’s first delve into perception.\nPerception is the ability to accurately experience the stream of information we’re exposed to. Our perception is influenced by our attention, which comes in two forms: lantern attention (broad and diffused) and spotlight attention (narrow and focused). While adults often rely on spotlight attention to filter and focus, children may utilize more lantern attention, taking in a wider array of stimuli.\nThese two states of attention are linked to the activity levels in the brain’s default mode network (DMN), which is most active during normal adult waking consciousness. To visualize this, think of adult brains as cities connected by roads: a network of sparsely connected nodes with a few high-traffic highways and many smaller roads within any given city.\nHaving experienced the world more extensively, adult brains, for better or worse, form shortcuts - high-traffic intercity highways. By leveraging their mental model of the world, these shortcuts help adults make sense of information and act quickly without exerting much energy. This is known as predictive coding. But these shortcuts also come at the cost of perceiving information less accurately.\nScientific studies indicate that psychedelics reduce DMN activity, increasing connectivity between brain regions (Pollan, 2018). This loosening of DMN control allows novel thinking patterns, as brain areas communicate more freely. It is akin to a shift from spotlight to lantern attention. For example, a study by Carhart-Harris et al. (2014) showed that psychedelics like psilocybin disrupt the DMN, enhancing sensory perception and cognitive flexibility.\nIn young children, the DMN is less developed and less active, correlating with a less pronounced sense of self. This reduced DMN activity means children are less focused on past and future selves. The “I” of small children is constrained to the present. They plan, daydream, reflect and ruminate less. This could explain why children exhibit spontaneous, uninhibited behavior and a heightened sense of wonder.\nChildren, like tripping adults possess a heightened ability to perceive reality, due to their less pronounced sense of self and more diffused attention. Temporarily disarming the default mode network may afford us a fresh look at the world free from the constraints of the mental models we have constructed over time.\n\n\nReferences\n\nPollan, Michael. How to Change Your Mind. Chapter Five: The Neuroscience (p. 348-395).\nhttps://www.frontiersin.org/articles/10.3389/fnhum.2014.00020/full\nhttps://en.wikipedia.org/wiki/Predictive_coding"
  }
]